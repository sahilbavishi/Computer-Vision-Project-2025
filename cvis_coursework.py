# -*- coding: utf-8 -*-
"""CVis - coursework.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hDZbpaSeIuCLYV1J4mHzKBg0c3orFfKF

# Computer Vision Coursework
## Image segmentation

Intro...
"""

# Setting code-behaviour variables
inCOLAB = True      # set to True if running on Google Colab
genVISUALS = False  # set to False in order to avoid time-consuming visualizations' genaration (images are instead displayed as pre-saved in 'Output/Visuals' folder)
dpi = 500           # dpi for .pdf-saved images visualization (with genVISUALS = False)
rePREPROC = False   # if True, the input images' resizing and augmentation are run, otherwise the saved outcomes are used
random_seed = 42



# Commented out IPython magic to ensure Python compatibility.
# Importing useful libraries
import os
from pathlib import Path
import pickle as pkl
from pdf2image import convert_from_path # (also install !apt-get install poppler-utils)
from IPython.display import display
import numpy as np
from PIL import Image
import seaborn as sns
import cv2
import tensorflow as tf
import albumentations as A
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from torchvision import transforms
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset

print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n',torch.cuda.is_available())

# Loading input data
input_folder_trainval = 'Data/Input/TrainVal'
input_trainval = [f for f in os.listdir(input_folder_trainval+'/color') if f.lower().endswith(('.jpg', '.jpeg'))]
input_trainval_labels = [f for f in os.listdir(input_folder_trainval+'/label') if f.lower().endswith(('.png'))]

input_folder_test = 'Data/Input/Test'
input_test = [f for f in os.listdir(input_folder_test+'/color') if f.lower().endswith(('.jpg', '.jpeg'))]
input_test_labels = [f for f in os.listdir(input_folder_test+'/label') if f.lower().endswith(('.png'))]

output_folder = 'Data/Output'
output_folder_resized = Path(os.path.join(output_folder, 'Resized'))
output_folder_augmented = Path(os.path.join(output_folder, 'Augmented'))

output_folder_resized_color = Path(os.path.join(output_folder_resized, 'color'))
output_folder_resized_label = Path(os.path.join(output_folder_resized, 'label'))
output_folder_augmented_color = Path(os.path.join(output_folder_augmented, 'color'))
output_folder_augmented_label = Path(os.path.join(output_folder_augmented, 'label'))

"""### 1. Dataset preprocessing and augmentation

The images are resized to the dimensions (H<sub>min</sub>, W<sub>min</sub>), thus to take the Q3 height and width size over all the images in the dataset; the instances will be processed in this format, then the output resized back to the original dimensions...

Furthermore...(data augmentation)
"""

# Gauging image sizes
if genVISUALS or rePREPROC:
  widths = []
  heights = []
  for img_file in input_trainval:
      with Image.open(Path(os.path.join(input_folder_trainval+'/color', img_file))) as img:
          width, height = img.size
          widths.append(width)
          heights.append(height)

# Visualizing histogram distribution of dimensions
if genVISUALS or rePREPROC:
  plt.figure(figsize=(12, 5))
  plt.subplot(1, 2, 1)
  plt.hist(widths, bins=20, color='lightblue', edgecolor='black')
  plt.xlabel("Width")
  plt.ylabel("Frequency")
  plt.title("Widths distribution")
  plt.subplot(1, 2, 2)
  plt.hist(heights, bins=20, color='skyblue', edgecolor='black')
  plt.xlabel("Height")
  plt.ylabel("Frequency")
  plt.title("Heights distribution")
  plt.tight_layout()
  plt.savefig(Path('Data/Output/Visuals/dimensions_histogram.pdf'))
  plt.show()
else:
  for image in convert_from_path(Path('Data/Output/Visuals/dimensions_histogram.pdf'), dpi=dpi):
    display(image)

# Visualizing boxplot distribution of dimensions
if genVISUALS or rePREPROC:
  plt.figure(figsize=(10, 6))
  data = {"Width": widths, "Height": heights}
  sns.boxplot(data=data, palette=['lightblue', 'skyblue'])
  plt.title("Boxplot of Widths and Heights")
  plt.ylabel("Pixels")
  plt.tight_layout()
  plt.savefig(Path('Data/Output/Visuals/dimensions_boxplot.pdf'))
  plt.show()
else:
  for image in convert_from_path(Path('Data/Output/Visuals/dimensions_boxplot.pdf'), dpi=dpi):
    display(image)

# Printing stats
print(f"Input set size: {len(input_trainval)}\n")
if rePREPROC:
  min_width, min_height = np.min(widths), np.min(heights)
  median_width, median_height = np.median(widths), np.median(heights)
  mean_width, mean_height = np.mean(widths), np.mean(heights)
  mode_width, mode_height = max(set(widths), key=widths.count), max(set(heights), key=heights.count)
  q3_width, q3_height = np.percentile(widths, 75), np.percentile(heights, 75)
  iqr_width = np.percentile(widths, 75) - np.percentile(widths, 25)
  iqr_height = np.percentile(heights, 75) - np.percentile(heights, 25)
  outlier_count_width = np.sum(widths > (q3_width + 1.5 * iqr_width))
  outlier_count_height = np.sum(heights > (q3_height + 1.5 * iqr_height))
  print(f"Min Size: {min_width}x{min_height}")
  print(f"Median Size: {median_width}x{median_height}")
  print(f"Mean Size: {mean_width:.2f}x{mean_height:.2f}")
  print(f"Mode Size: {mode_width}x{mode_height}")
  print(f"Q3 Size: {q3_width}x{q3_height}")
  print(f"Outliers in width: {outlier_count_width}")
  print(f"Outliers in height: {outlier_count_height}")
else:
  print("Q3 width and height values (both 500 pixels) were chosen for resizing.")

"""#### a) Resizing"""

# Resizing images (to Q3 width, Q3 height)
if rePREPROC:
  imgResize = (int(q3_width), int(q3_height))
  widthsNP = np.array(widths)
  heightsNP = np.array(heights)
  i = 0
  for img_file in input_trainval:
      with Image.open(Path(os.path.join(input_folder_trainval+'/color', img_file))) as img:
          img_resized = img.resize(imgResize, Image.Resampling.LANCZOS)
          if img_resized.mode == "RGBA":
            img_resized = img_resized.convert("RGB")
          img_resized.save(os.path.join(output_folder_resized_color, img_file), format="JPEG")
          i += 1
  print(f"{i} images resized to {int(q3_width)}x{int(q3_height)} and saved in {output_folder_resized_color}")

# Resizing labels
  imgResize = (int(q3_width), int(q3_height))
  widthsNP = np.array(widths)
  heightsNP = np.array(heights)
  i = 0
  for img_file in input_trainval_labels:
      with Image.open(Path(os.path.join(input_folder_trainval+'/label', img_file))) as img:
          img_resized = img.resize(imgResize, Image.Resampling.LANCZOS)
          if img_resized.mode == "RGBA":
            img_resized = img_resized.convert("RGB")
          img_resized.save(os.path.join(output_folder_resized_label, img_file), format="PNG")
          i += 1
  print(f"{i} labels resized to {int(q3_width)}x{int(q3_height)} and saved in {output_folder_resized_label}")

else:
  print("Using previously resized images and labels (500x500).")
  imgResize = (500, 500)

"""#### b) Augmenting dataset

TODO: add additional augmenting techniques from the following code
"""

# # b) Augmenting dataset (inspired to U-Net)
# if rePREPROC:
#   augmentation = A.Compose([
#       A.HorizontalFlip(p=0.5),
#       A.VerticalFlip(p=0.5),
#       A.RandomRotate90(p=0.5),
#       A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),
#       A.RandomBrightnessContrast(p=0.2),
#       A.GaussianBlur(blur_limit=(3, 7), p=0.2),
#       A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),
#   ])

#   i = 0
#   for img_file in os.listdir(output_folder_resized_color):
#       img_path = os.path.join(output_folder_resized_color, img_file)
#       img = cv2.imread(img_path)
#       if img is None:
#           continue
#       img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # converting BGR to RGB
#       augmented = augmentation(image=img)['image']
#       output_path = os.path.join(output_folder_augmented_color, f'aug_{img_file}')
#       Image.fromarray(augmented).save(output_path)
#       i += 1

#   print(f"{i} images augmented, output saved in {output_folder_augmented_color}")
# else:
#   print("Using previously augmented data.")

# b) Augmenting dataset (inspired by U-Net)
if rePREPROC:
    augmentation = A.Compose([
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5, always_apply=True)  # Ensure it's always applied
    ], additional_targets={'mask': 'mask'})  # Use 'mask' instead of 'image' to avoid interpolation issues

    i = 0
    for img_file in os.listdir(output_folder_resized_color):
        img_path = os.path.join(output_folder_resized_color, img_file)
        mask_filename = os.path.splitext(img_file)[0] + ".png"  # Replace .jpeg with .png
        mask_path = os.path.join(output_folder_resized_label, mask_filename)

        # Load image and mask
        img = cv2.imread(img_path)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask in grayscale

        if img is None or mask is None:
            print(f"Skipping {img_file} as corresponding image or mask is missing.")
            continue

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB

        # Apply augmentation
        augmented = augmentation(image=img, mask=mask)
        aug_img, aug_mask = augmented['image'], augmented['mask']

        # Ensure mask remains categorical by reapplying np.uint8
        aug_mask = np.round(aug_mask).astype(np.uint8)

        # Ensure mask retains only valid values
        unique_values = np.unique(mask)
        for value in np.unique(aug_mask):
            if value not in unique_values:
                aug_mask[aug_mask == value] = 0  # Convert any unexpected values to 0

        # Save augmented image
        output_img_path = os.path.join(output_folder_augmented_color, f'aug_{img_file}')
        Image.fromarray(aug_img).save(output_img_path, "JPEG")  # Save as JPEG

        # Save augmented mask
        output_mask_filename = f'aug_{mask_filename}'  # Ensure naming consistency
        output_mask_path = os.path.join(output_folder_augmented_label, output_mask_filename)
        Image.fromarray(aug_mask).save(output_mask_path, "PNG")  # Save as PNG

        i += 1

    print(f"{i} images and masks augmented, output saved in {output_folder_augmented_color} & {output_folder_augmented_label}")
else:
    print("Using previously augmented data.")

"""#### c) Preparing datasets"""

# Preparing train, valid and test sets
validSize = 0.2
batchSize = 16
imgChannels = 3
inputSize = (imgResize[0], imgResize[1], imgChannels)

trainval_images = [os.path.join(output_folder_augmented_color, f) for f in os.listdir(output_folder_augmented_color) if f.endswith('.jpg')]
trainval_masks = [os.path.join(output_folder_augmented_label, f) for f in os.listdir(output_folder_augmented_label) if f.endswith('.png')]
train_images, val_images, train_masks, val_masks = train_test_split(trainval_images, trainval_masks, test_size=validSize, random_state=random_seed)

test_images = [os.path.join(input_folder_test, 'color', f) for f in os.listdir(os.path.join(input_folder_test, 'color')) if f.endswith('.jpg')]
test_masks = [os.path.join(input_folder_test, 'label', f) for f in os.listdir(os.path.join(input_folder_test, 'label')) if f.endswith('.png')]

def preprocess_image(image_path, target_size=imgResize):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=imgChannels)
    image = tf.image.resize(image, target_size)
    image = image / 255.0 # normalizing to [0, 1]
    return image

def preprocess_mask(mask_path, target_size=imgResize):
    mask_path = tf.strings.join(['', mask_path]) # converting to string if tensor
    mask_extension = tf.strings.split(mask_path, '.')[-1] # handling .jpg as grayscale or .png
    mask_extension = tf.strings.lower(mask_extension)
    mask = tf.io.read_file(mask_path)
    if mask_extension == 'jpg' or mask_extension == 'jpeg':
        mask = tf.image.decode_jpeg(mask, channels=imgChannels)
    else:
        mask = tf.image.decode_png(mask, channels=imgChannels)
    mask = tf.image.resize(mask, target_size, method='nearest')
    mask = tf.squeeze(mask)
    mask = tf.cast(mask, tf.int32)
    return mask

def create_dataset(image_paths, mask_paths, batch_size=batchSize, shuffle=True):
    def load_and_preprocess(image_path, mask_path):
        image = preprocess_image(image_path)
        mask = preprocess_mask(mask_path)
        return image, mask
    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))
    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    if shuffle:
        dataset = dataset.shuffle(buffer_size=len(image_paths))
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset

train_dataset = create_dataset(train_images, train_masks, batch_size=batchSize, shuffle=True)
val_dataset = create_dataset(val_images, val_masks, batch_size=batchSize, shuffle=False)
test_dataset = create_dataset(test_images, test_masks, batch_size=batchSize, shuffle=False)

print(f"Train set size: {len(train_images)} ({(1-validSize)*100}%)")
print(f"Valid set size: {len(val_images)} ({(validSize)*100}%)")
print(f" Test set size: {len(test_images)}")
print(f"\nInput dimension: {inputSize}")
print(f"Batches' size: {batchSize}")

# Preparing train, valid and test sets

# NOW USING NOT AUGMENTED INPUT FOR TRAINING: _____
#image_folder = input_folder_trainval + '/color'  # -> output_folder_augmented
#mask_folder = input_folder_trainval + '/label'   # TODO: augmented masks folder
# - - - - - - - - - - - - - - - - - - - - - - - - -

# validSize = 0.2
# batchSize = 16
# imgChannels = 3
# inputSize = (imgResize[0], imgResize[1], imgChannels)

# trainval_images = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.jpg')])
# trainval_masks = sorted([os.path.join(mask_folder, f.replace('.jpg', '.png')) for f in os.listdir(image_folder) if f.endswith('.jpg')])
# train_images, val_images, train_masks, val_masks = train_test_split(trainval_images, trainval_masks, test_size=validSize, random_state=random_seed)

# test_images = sorted([os.path.join(input_folder_test, 'color', f) for f in os.listdir(os.path.join(input_folder_test, 'color')) if f.endswith('.jpg')])
# test_masks = sorted([os.path.join(input_folder_test, 'label', f.replace('.jpg', '.png')) for f in os.listdir(os.path.join(input_folder_test, 'label')) if f.endswith('.png')])

# # Converting to TensorFlow datasets
# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_masks))
# val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_masks))
# test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_masks))

# def load_image_and_mask(image_path, mask_path):
#     image = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=imgChannels)
#     image = tf.image.resize(image, imgResize)
#     image = image / 255.0 # normalizing to [0, 1]
#     mask = tf.image.decode_png(tf.io.read_file(mask_path), channels=1)
#     mask = tf.image.resize(mask, imgResize, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
#     return image, mask

# # Applying resizing and normalization
# train_dataset = train_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)
# val_dataset = val_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)
# test_dataset = test_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)

# # Prefetching
# train_dataset = train_dataset.batch(batchSize).prefetch(tf.data.AUTOTUNE)
# val_dataset = val_dataset.batch(batchSize).prefetch(tf.data.AUTOTUNE)
# test_dataset = test_dataset.batch(batchSize).prefetch(tf.data.AUTOTUNE)

# Preparing sample test image
sampleImg = Image.open("Data/Input/TrainVal/color/Birman_159.jpg").convert('RGB')
sampleLabel = Image.open("Data/Input/TrainVal/label/Birman_159.png").convert('RGB')

"""### 2. Segmentation network

Description...

#### a) UNet-based end-to-end NN (TO FIX)
"""

# Building and training the model
from tensorflow.keras import layers, models, backend as K

classesNum = 3 # number of output classes
epochsNum = 2 # number of training epochs

def crop_and_concat(target, reference):
    """Ensures target tensor matches the shape of reference tensor before concatenation."""
    target_shape = K.int_shape(target)
    reference_shape = K.int_shape(reference)

    if target_shape is None or reference_shape is None:
        raise ValueError("Input tensors must have defined shapes.")

    height_diff = target_shape[1] - reference_shape[1]
    width_diff = target_shape[2] - reference_shape[2]

    # Ensure valid cropping values (avoid negative numbers)
    crop_h = (max(height_diff // 2, 0), max(height_diff - (height_diff // 2), 0))
    crop_w = (max(width_diff // 2, 0), max(width_diff - (width_diff // 2), 0))

    # Apply cropping only if needed
    if crop_h != (0, 0) or crop_w != (0, 0):
        target = layers.Cropping2D(cropping=(crop_h, crop_w))(target)

    # Resize if still mismatched (Failsafe)
    target = layers.Resizing(reference_shape[1], reference_shape[2])(target)

    return layers.Concatenate()([target, reference])


def unet_model(input_size=inputSize, num_classes=classesNum):
    inputs = layers.Input(input_size)

    # Encoder
    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)
    p1 = layers.MaxPooling2D((2, 2))(c1)

    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)
    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)
    p2 = layers.MaxPooling2D((2, 2))(c2)

    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)
    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)
    p3 = layers.MaxPooling2D((2, 2))(c3)

    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)
    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)

    # Decoder
    u5 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)
    u5 = crop_and_concat(u5, c3)
    c5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u5)
    c5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c5)

    u6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)
    u6 = crop_and_concat(u6, c2)
    c6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u6)
    c6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c6)

    u7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)
    u7 = crop_and_concat(u7, c1)
    c7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u7)
    c7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c7)

    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(c7)

    model = models.Model(inputs, outputs)
    return model

# Create and compile the model
model = unet_model(input_size=inputSize, num_classes=classesNum)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Training
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=epochsNum,
    batch_size=batchSize
)
with open('Data/Output/Models/unet_history.pkl', 'wb') as f:
    pkl.dump(history, f)
with open('Data/Output/Models/unet_model.pkl', 'wb') as f:
    pkl.dump(model, f)

# Evaluation
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

# Testing the model (TO FIX)
image = preprocess_image('path_to_image.jpg')
mask = preprocess_mask('path_to_mask.png')
# Visualizing predictions
def visualize_prediction(image, mask, prediction):
    plt.figure(figsize=(15, 5))

    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(image)
    plt.title('Original Image')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(mask, cmap='jet')
    plt.title('Ground Truth Mask')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(np.argmax(prediction, axis=-1), cmap='jet')
    plt.title('Predicted Mask')

    plt.show()

image, mask = next(iter(test_dataset))
prediction = model.predict(tf.expand_dims(image, axis=0))
visualize_prediction(image, mask, prediction[0])

"""#### b) Autoencoders (TO FIX)"""

# This is the encoder part that learns on raw images and no labels

# Define the Autoencoder
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # 500 -> 250
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # 250 -> 125
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=5, stride=5)   # 125 -> 25
        )
        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=5),  # 25 -> 125
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),   # 125 -> 250
            nn.ReLU(),
            nn.ConvTranspose2d(64, 3, kernel_size=2, stride=2),     # 250 -> 500
            nn.Sigmoid()  # Output is in the range [0, 1]
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Custom Dataset for loading images
class ImageDataset(Dataset):
    def __init__(self, image_files, transform=None, max_images=100):
        #self.image_folder = image_folder
        #self.image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        self.image_files = image_files  # Limit to first 100 images
        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image

# Transformations
transform = transforms.Compose([
    transforms.Resize((500, 500)),  # Resize images to 500x500
    transforms.ToTensor()           # Convert to tensor
])

# Load dataset with first 100 images
dataset = ImageDataset(image_files=train_images, transform=transform, max_images=100)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # Reduce batch size to avoid memory issues

# Initialize model, loss, and optimizer
autoencoder = Autoencoder()
criterion = nn.MSELoss()  # Mean Squared Error for reconstruction
optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)

# Training the Autoencoder
num_epochs = 10
for epoch in range(num_epochs):
    print(f"Epoch [{epoch+1}/{num_epochs}] Running:")
    for batch in dataloader:
        optimizer.zero_grad()
        outputs = autoencoder(batch)
        loss = criterion(outputs, batch)
        loss.backward()
        optimizer.step()
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")
    # Save only the encoder after training
    torch.save(autoencoder.encoder.state_dict(), 'Data/Output/Models/encoder.pth')

# # This is the decoder part that learns with the fixed encoder above
# # Define the Segmentation Decoder
# class SegmentationDecoder(nn.Module):
#     def __init__(self, encoder):
#         super(SegmentationDecoder, self).__init__()
#         self.encoder = encoder
#         # Freeze the encoder
#         for param in self.encoder.parameters():
#             param.requires_grad = False
#         # Segmentation Decoder
#         self.decoder = nn.Sequential(
#             nn.ConvTranspose2d(256, 128, kernel_size=5, stride=5),  # 25 -> 125
#             nn.ReLU(),
#             nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),   # 125 -> 250
#             nn.ReLU(),
#             nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),    # 250 -> 500
#             nn.Conv2d(32, 1, kernel_size=1),  # Output a single channel for binary segmentation
#             nn.Sigmoid()  # Output is in the range [0, 1]
#         )

#     def forward(self, x):
#         x = self.encoder(x)
#         x = self.decoder(x)
#         return x

# # Custom Dataset for loading images and labels
# class SegmentationDataset(Dataset):
#     def __init__(self, image_files, label_files, transform=None, max_images=100):
#         #self.image_folder = image_folder
#         #self.label_folder = label_folder
#         #self.image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
#         self.image_files = image_files[:max_images]
#         self.label_files = label_files[:max_images]
#         self.transform = transform

#     def __len__(self):
#         return len(self.image_files)

#     def __getitem__(self, idx):
#         img_path = self.image_files[idx]
#         label_path = self.image_files[idx].replace('.jpg', '.png').replace('color', 'label')
#         image = Image.open(img_path).convert('RGB')
#         label = Image.open(label_path).convert('L')  # Grayscale for binary labels
#         if self.transform:
#             image = self.transform(image)
#             label = self.transform(label)
#         return image, label

# # Transformations
# transform = transforms.Compose([
#     transforms.Resize((500, 500)),  # Resize images to 500x500
#     transforms.ToTensor()           # Convert to tensor
# ])

# # Load segmentation dataset with first 100 images
# segmentation_dataset = SegmentationDataset(
#     image_files=train_images,
#     label_files=train_masks,
#     transform=transform,
#     max_images=100
# )
# segmentation_dataloader = DataLoader(segmentation_dataset, batch_size=16, shuffle=True)

# # Load the pre-trained autoencoder
# autoencoder = Autoencoder()
# autoencoder.load_state_dict(torch.load('Data/Output/Models/autoencoder.pth'))

# # Initialize the segmentation model
# segmentation_model = SegmentationDecoder(autoencoder.encoder)

# # Define loss and optimizer for segmentation
# segmentation_criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for segmentation
# segmentation_optimizer = optim.Adam(segmentation_model.decoder.parameters(), lr=0.001)

# # Training the Segmentation Model
# num_epochs = 10
# for epoch in range(num_epochs):
#     print(f"Epoch [{epoch+1}/{num_epochs}] Running:")
#     for images, labels in segmentation_dataloader:
#         segmentation_optimizer.zero_grad()
#         outputs = segmentation_model(images)
#         loss = segmentation_criterion(outputs, labels)
#         loss.backward()
#         segmentation_optimizer.step()
#     print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

# # Save the segmentation model
# torch.save(segmentation_model.state_dict(), 'Data/Output/Models/segmentation_model.pth')

# This is the decoder part that learns with the fixed encoder above
    # Define the Segmentation Decoder
    class SegmentationDecoder(nn.Module):
        def __init__(self, encoder):
            super(SegmentationDecoder, self).__init__()
            self.encoder = encoder
            # Freeze the encoder
            for param in self.encoder.parameters():
                param.requires_grad = False
            # Segmentation Decoder
            self.decoder = nn.Sequential(
                nn.ConvTranspose2d(256, 128, kernel_size=5, stride=5),  # 25 -> 125
                nn.ReLU(),
                nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),   # 125 -> 250
                nn.ReLU(),
                nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),    # 250 -> 500
                nn.Conv2d(32, 3, kernel_size=1),  # Output 3 channels (one per class)
            )


        def forward(self, x):
            x = self.encoder(x)
            x = self.decoder(x)
            return x

    # Custom Dataset for loading images and labels
    class SegmentationDataset(Dataset):
        def __init__(self, image_files, label_files, transform=None, max_images=100):
            self.image_files = image_files
            self.label_files = label_files
            self.transform = transform

        def __len__(self):
            return len(self.image_files)

        def __getitem__(self, idx):
            img_path = self.image_files[idx]
            label_path = self.image_files[idx].replace('.jpg', '.png').replace('color', 'label')
            image = Image.open(img_path).convert('RGB')
            label = Image.open(label_path).convert('L')  # Load label as grayscale

            if self.transform:
                image = self.transform(image)

            # Convert label to tensor and map pixel values to class indices
            label = torch.tensor(np.array(label), dtype=torch.long)  # Convert label to tensor
            label = torch.where(label == 38, 1, label)  # Map 38 to class index 1
            label = torch.where(label == 75, 1, label)  # If using 75, adjust here
            label = torch.where(label == 255, 2, label)  # Map 255 to class index 2
            label = torch.where(label == 0, 0, label)  # Ensure 0 stays as class 0
            return image, label


    # Transformations
    transform = transforms.Compose([
        transforms.Resize((500, 500)),  # Resize images to 500x500
        transforms.ToTensor()           # Convert to tensor
    ])

    # Load segmentation dataset with first 100 images
    segmentation_dataset = SegmentationDataset(
        image_files=train_images,
        label_files=train_masks,
        transform=transform,
        max_images=100
    )
    segmentation_dataloader = DataLoader(segmentation_dataset, batch_size=16, shuffle=True)

    # Load the pre-trained autoencoder
 # Load only the encoder
    encoder = Autoencoder().encoder
    encoder.load_state_dict(torch.load('Data/Output/Models/encoder.pth'))

    # Initialize the segmentation model
    segmentation_model = SegmentationDecoder(encoder)

    # Define loss and optimizer for segmentation
    segmentation_criterion = nn.CrossEntropyLoss()
    segmentation_optimizer = optim.Adam(segmentation_model.decoder.parameters(), lr=0.001)

    # Training the Segmentation Model
    num_epochs = 4
    for epoch in range(num_epochs):
        print(f"Epoch [{epoch+1}/{num_epochs}] Running:")
        for images, labels in segmentation_dataloader:
            segmentation_optimizer.zero_grad()
            outputs = segmentation_model(images)  # Shape: (Batch, 3, H, W)

            # Ensure labels are (Batch, H, W) and of type Long
            labels = labels.squeeze(1).long()  # Ensure shape is (Batch, H, W)

            loss = segmentation_criterion(outputs, labels)
            loss.backward()
            segmentation_optimizer.step()
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")
        torch.save(segmentation_model.state_dict(), f'Data/Output/Models/segmentation_model_epoch_{epoch+1}.pth')

# This is helping us see what the model is doing

import matplotlib.pyplot as plt
import torch
from torchvision import transforms
from PIL import Image
import os


# Load the trained segmentation model
segmentation_model = SegmentationDecoder(autoencoder.encoder)
segmentation_model.load_state_dict(torch.load('Data/Output/Models/segmentation_model_epoch_4.pth'))
segmentation_model.eval()  # Set model to evaluation mode

# Define the transformation (must match training transform)
transform = transforms.Compose([
    transforms.Resize((500, 500)),
    transforms.ToTensor()
])

# Function to make predictions and visualize results
def visualize_segmentation(image, label):
    input_tensor = transform(image).unsqueeze(0)  # Add batch dimension

    # Forward pass through the model
    with torch.no_grad():
        output = segmentation_model(input_tensor)

    # Convert model output to numpy for visualization
    prediction = output.squeeze().cpu().numpy()  # Remove batch & channel dimensions
    label = transform(label).squeeze().cpu().numpy()  # Process label similarly

    # Plot the input, ground truth, and prediction
    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    axes[0].imshow(image)
    axes[0].set_title("Input Image")
    axes[0].axis("off")

    axes[1].imshow(label, cmap='gray')
    axes[1].set_title("Ground Truth Label")
    axes[1].axis("off")

    axes[2].imshow(prediction, cmap='gray')
    axes[2].set_title("Model Prediction")
    axes[2].axis("off")

    plt.show()

# Preparing sample test image
sampleImg1 = Image.open("Data/Output/Resized/color/Birman_159.jpg").convert('RGB')
sampleLabel1 = Image.open("Data/Output/Resized/label/Birman_159.png").convert('L') # (grayscale)

visualize_segmentation(sampleImg1, sampleLabel1)

"""#### c) CLIP (incomplete)"""

from transformers import CLIPProcessor, CLIPModel
import torch
from PIL import Image

'''
17 minutes to run this code block
'''

# Load the CLIP model and processor from Hugging Face
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Load an image and process it
def extract_clip_features(image_path):
    image = Image.open(image_path)
    inputs = processor(images=image, return_tensors="pt", padding=True).to(device)

    # Extract features from CLIP model
    with torch.no_grad():
        features = model.get_image_features(**inputs)

    # Normalize features
    features = features / features.norm(p=2, dim=-1, keepdim=True)
    return features

clip_features_trainval = []

# Clip_features_trainval is a list containing 3680 tensors of shape 1, 512
# These tensors must be used as input into the models to make segment.

for image in trainval_images:
    clip_features = extract_clip_features(image)
    clip_features_trainval.append(clip_features)

len(clip_features_trainval)

# Example usage for a single image
image_path = "Data/Input/TrainVal/color/Birman_159.jpg"  # Replace with your image path
clip_features = extract_clip_features(image_path)
print(clip_features.shape)  # Output the shape of the extracted features

# This code block takes 55 mins to run
import os
import torch
from PIL import Image
import torch.nn as nn
import torchvision.transforms as transforms

# Segmentation model class with CLIP features
class SegmentationWithCLIP(nn.Module):
    def __init__(self, segmentation_model, clip_features_dim):
        super(SegmentationWithCLIP, self).__init__()
        self.segmentation_model = segmentation_model
        self.clip_fc = nn.Linear(clip_features_dim, 256)  # Adjust the dimension as needed

    def forward(self, x, clip_features):
        # Get segmentation predictions
        seg_output = self.segmentation_model(x)

        # Process CLIP features (resize or adapt as necessary)
        clip_features = self.clip_fc(clip_features)

        # Combine segmentation output with CLIP features
        combined_features = torch.cat([seg_output, clip_features], dim=1)

        # Final segmentation prediction
        final_segmentation = self.final_layer(combined_features)
        return final_segmentation

# Example of loading and processing training data with CLIP features
clip_features_trainval = []

for image_name in input_trainval:
    image_path = Path(os.path.join(input_folder_trainval, 'color', image_name))
    clip_features = extract_clip_features(image_path)
    clip_features_trainval.append(clip_features)

# # Example of processing test data with CLIP features
# clip_features_test = []

# for image_name in input_test:
#     image_path = Path(os.path.join(input_folder_test, 'color', image_name))
#     clip_features = extract_clip_features(image_path)
#     clip_features_test.append(clip_features)

# Assume segmentation_model is defined elsewhere in your code
# Initialize SegmentationWithCLIP with your model and appropriate CLIP feature dimensions


segmentation_model = None  # Replace with your segmentation model
clip_features_dim = 512  # CLIP features are typically 512-dimensional
segmentation_with_clip = SegmentationWithCLIP(segmentation_model, clip_features_dim)

