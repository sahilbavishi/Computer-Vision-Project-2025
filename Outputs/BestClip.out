2025-03-29 16:10:01.447305: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-29 16:10:01.447446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-29 16:10:01.450153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-29 16:10:01.464922: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-29 16:10:05.491883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/s2677266/miniconda3/envs/mlsys/lib/python3.10/site-packages/albumentations/check_version.py:107: UserWarning: Error fetching version info <urlopen error timed out>
  data = fetch_version_info()
/home/s2677266/miniconda3/envs/mlsys/lib/python3.10/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a "-quickgelu" suffix or enable with a flag.
  warnings.warn(
/home/s2677266/CVis/Clip.py:276: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Input set size: 3680

Q3 width and height values (both 500 pixels) were chosen for resizing.
Using previously resized images and labels (500x500).
Using previously augmented data.
Train set size: 7788 (80.0%)
Valid set size: 1948 (20.0%)
 Test set size: 3710

Input dimension: (256, 256, 3)
Batches' size: 4
Extracting Clip Features
*****************************************************
Loaded 9736 CLIP feature tensors.
*****************************************************
Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]/home/s2677266/CVis/Clip.py:293: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
Training Progress:   1%|          | 1/100 [06:16<10:21:59, 376.96s/it]Training Progress:   2%|▏         | 2/100 [12:25<10:07:15, 371.79s/it]Training Progress:   3%|▎         | 3/100 [18:29<9:55:34, 368.39s/it] Training Progress:   4%|▍         | 4/100 [24:49<9:56:45, 372.97s/it]Training Progress:   5%|▌         | 5/100 [31:05<9:52:23, 374.14s/it]Training Progress:   6%|▌         | 6/100 [37:11<9:41:58, 371.47s/it]Training Progress:   7%|▋         | 7/100 [43:20<9:34:10, 370.43s/it]Training Progress:   8%|▊         | 8/100 [49:31<9:28:19, 370.64s/it]Training Progress:   9%|▉         | 9/100 [55:45<9:23:45, 371.71s/it]Training Progress:  10%|█         | 10/100 [1:01:57<9:17:37, 371.75s/it]Training Progress:  11%|█         | 11/100 [1:08:04<9:09:35, 370.51s/it]Training Progress:  12%|█▏        | 12/100 [1:14:10<9:01:06, 368.94s/it]Training Progress:  13%|█▎        | 13/100 [1:20:26<8:58:03, 371.08s/it]Training Progress:  14%|█▍        | 14/100 [1:26:32<8:49:44, 369.59s/it]Training Progress:  15%|█▌        | 15/100 [1:32:40<8:42:59, 369.17s/it]Training Progress:  16%|█▌        | 16/100 [1:39:03<8:42:47, 373.42s/it]Training Progress:  17%|█▋        | 17/100 [1:45:11<8:33:58, 371.55s/it]Training Progress:  18%|█▊        | 18/100 [1:51:14<8:24:31, 369.17s/it]Training Progress:  19%|█▉        | 19/100 [1:57:33<8:22:16, 372.06s/it]Training Progress:  20%|██        | 20/100 [2:03:42<8:14:50, 371.14s/it]Training Progress:  21%|██        | 21/100 [2:09:47<8:06:06, 369.20s/it]Training Progress:  22%|██▏       | 22/100 [2:16:03<8:02:39, 371.28s/it]Training Progress:  23%|██▎       | 23/100 [2:22:21<7:59:15, 373.45s/it]Training Progress:  24%|██▍       | 24/100 [2:28:27<7:50:06, 371.13s/it]Training Progress:  25%|██▌       | 25/100 [2:34:46<7:46:59, 373.59s/it]Training Progress:  26%|██▌       | 26/100 [2:40:58<7:40:06, 373.05s/it]Training Progress:  27%|██▋       | 27/100 [2:47:13<7:34:24, 373.48s/it]Training Progress:  28%|██▊       | 28/100 [2:53:30<7:29:29, 374.58s/it]Training Progress:  29%|██▉       | 29/100 [2:59:51<7:25:32, 376.52s/it]Training Progress:  30%|███       | 30/100 [3:05:58<7:15:54, 373.64s/it]Training Progress:  31%|███       | 31/100 [3:12:10<7:09:02, 373.08s/it]Training Progress:  32%|███▏      | 32/100 [3:18:27<7:04:27, 374.53s/it]Training Progress:  33%|███▎      | 33/100 [3:24:33<6:55:05, 371.73s/it]Training Progress:  34%|███▍      | 34/100 [3:30:44<6:48:37, 371.48s/it]Training Progress:  35%|███▌      | 35/100 [3:36:51<6:41:01, 370.17s/it]Training Progress:  36%|███▌      | 36/100 [3:43:14<6:39:00, 374.07s/it]Training Progress:  37%|███▋      | 37/100 [3:49:30<6:33:29, 374.75s/it]Training Progress:  38%|███▊      | 38/100 [3:55:45<6:27:14, 374.76s/it]Training Progress:  39%|███▉      | 39/100 [4:01:51<6:18:23, 372.19s/it]Training Progress:  40%|████      | 40/100 [4:08:02<6:11:43, 371.72s/it]Training Progress:  41%|████      | 41/100 [4:14:18<6:06:59, 373.21s/it]Training Progress:  42%|████▏     | 42/100 [4:20:23<5:58:07, 370.47s/it]Training Progress:  43%|████▎     | 43/100 [4:26:34<5:52:18, 370.85s/it]Training Progress:  44%|████▍     | 44/100 [4:32:54<5:48:27, 373.36s/it]Training Progress:  45%|████▌     | 45/100 [4:39:00<5:40:13, 371.16s/it]Training Progress:  46%|████▌     | 46/100 [4:45:12<5:34:30, 371.67s/it]Training Progress:  47%|████▋     | 47/100 [4:51:18<5:26:38, 369.79s/it]Training Progress:  48%|████▊     | 48/100 [4:57:25<5:19:42, 368.90s/it]Training Progress:  49%|████▉     | 49/100 [5:03:44<5:16:07, 371.92s/it]Training Progress:  50%|█████     | 50/100 [5:09:51<5:08:52, 370.66s/it]Training Progress:  51%|█████     | 51/100 [5:15:55<5:01:05, 368.68s/it]Training Progress:  52%|█████▏    | 52/100 [5:22:12<4:56:47, 370.98s/it]Training Progress:  53%|█████▎    | 53/100 [5:28:19<4:49:41, 369.82s/it]Training Progress:  54%|█████▍    | 54/100 [5:34:25<4:42:37, 368.65s/it]Training Progress:  55%|█████▌    | 55/100 [5:40:39<4:37:44, 370.31s/it]Training Progress:  56%|█████▌    | 56/100 [5:46:45<4:30:33, 368.94s/it]Training Progress:  57%|█████▋    | 57/100 [5:52:49<4:23:19, 367.42s/it]Training Progress:  58%|█████▊    | 58/100 [5:59:04<4:18:55, 369.88s/it]Training Progress:  59%|█████▉    | 59/100 [6:05:13<4:12:35, 369.64s/it]Training Progress:  60%|██████    | 60/100 [6:11:23<4:06:25, 369.63s/it]Training Progress:  61%|██████    | 61/100 [6:17:45<4:02:37, 373.27s/it]Training Progress:  62%|██████▏   | 62/100 [6:23:56<3:55:58, 372.59s/it]Training Progress:  63%|██████▎   | 63/100 [6:30:04<3:48:56, 371.26s/it]Training Progress:  64%|██████▍   | 64/100 [6:36:18<3:43:16, 372.12s/it]Training Progress:  65%|██████▌   | 65/100 [6:42:38<3:38:27, 374.51s/it]Training Progress:  66%|██████▌   | 66/100 [6:48:46<3:31:10, 372.67s/it]Training Progress:  67%|██████▋   | 67/100 [6:54:52<3:23:44, 370.44s/it]Training Progress:  68%|██████▊   | 68/100 [7:00:56<3:16:35, 368.62s/it]Training Progress:  69%|██████▉   | 69/100 [7:07:02<3:09:59, 367.74s/it]Training Progress:  70%|███████   | 70/100 [7:13:14<3:04:36, 369.22s/it]Training Progress:  71%|███████   | 71/100 [7:19:28<2:59:02, 370.42s/it]Training Progress:  72%|███████▏  | 72/100 [7:25:35<2:52:23, 369.41s/it]Training Progress:  73%|███████▎  | 73/100 [7:31:44<2:46:15, 369.48s/it]Training Progress:  74%|███████▍  | 74/100 [7:37:57<2:40:33, 370.51s/it]Training Progress:  75%|███████▌  | 75/100 [7:44:00<2:33:23, 368.12s/it]Training Progress:  76%|███████▌  | 76/100 [7:50:16<2:28:11, 370.49s/it]Training Progress:  77%|███████▋  | 77/100 [7:56:21<2:21:25, 368.95s/it]Training Progress:  78%|███████▊  | 78/100 [8:02:25<2:14:43, 367.45s/it]Training Progress:  79%|███████▉  | 79/100 [8:08:37<2:09:04, 368.77s/it]Training Progress:  80%|████████  | 80/100 [8:14:46<2:02:59, 368.97s/it]Training Progress:  81%|████████  | 81/100 [8:20:57<1:56:58, 369.42s/it]Training Progress:  82%|████████▏ | 82/100 [8:27:17<1:51:48, 372.71s/it]Training Progress:  83%|████████▎ | 83/100 [8:33:28<1:45:24, 372.00s/it]Training Progress:  84%|████████▍ | 84/100 [8:39:34<1:38:43, 370.24s/it]Training Progress:  85%|████████▌ | 85/100 [8:45:50<1:32:59, 371.94s/it]Training Progress:  86%|████████▌ | 86/100 [8:52:05<1:27:01, 372.95s/it]Training Progress:  87%|████████▋ | 87/100 [8:58:21<1:20:59, 373.79s/it]Training Progress:  88%|████████▊ | 88/100 [9:04:36<1:14:52, 374.35s/it]Training Progress:  89%|████████▉ | 89/100 [9:10:54<1:08:47, 375.21s/it]Training Progress:  90%|█████████ | 90/100 [9:16:57<1:01:57, 371.71s/it]Training Progress:  91%|█████████ | 91/100 [9:23:08<55:43, 371.53s/it]  Training Progress:  92%|█████████▏| 92/100 [9:29:20<49:31, 371.47s/it]Training Progress:  93%|█████████▎| 93/100 [9:35:26<43:10, 370.08s/it]Training Progress:  94%|█████████▍| 94/100 [9:41:38<37:03, 370.65s/it]Training Progress:  95%|█████████▌| 95/100 [9:47:46<30:48, 369.74s/it]Training Progress:  96%|█████████▌| 96/100 [9:53:52<24:34, 368.68s/it]Training Progress:  97%|█████████▋| 97/100 [10:00:02<18:26, 368.96s/it]Training Progress:  98%|█████████▊| 98/100 [10:06:09<12:16, 368.43s/it]Training Progress:  99%|█████████▉| 99/100 [10:12:16<06:07, 367.99s/it]Training Progress: 100%|██████████| 100/100 [10:18:34<00:00, 371.02s/it]Training Progress: 100%|██████████| 100/100 [10:18:34<00:00, 371.15s/it]
Epoch [1/100] Running:
Epoch [1/100], Loss: 0.8346
Epoch [2/100] Running:
Epoch [2/100], Loss: 0.6849
Epoch [3/100] Running:
Epoch [3/100], Loss: 0.6161
Epoch [4/100] Running:
Epoch [4/100], Loss: 0.5874
Epoch [5/100] Running:
Epoch [5/100], Loss: 0.5664
Epoch [6/100] Running:
Epoch [6/100], Loss: 0.5270
Epoch [7/100] Running:
Epoch [7/100], Loss: 0.5175
Epoch [8/100] Running:
Epoch [8/100], Loss: 0.5085
Epoch [9/100] Running:
Epoch [9/100], Loss: 0.5004
Epoch [10/100] Running:
Epoch [10/100], Loss: 0.4944
Epoch [11/100] Running:
Epoch [11/100], Loss: 0.4744
Epoch [12/100] Running:
Epoch [12/100], Loss: 0.4705
Epoch [13/100] Running:
Epoch [13/100], Loss: 0.4676
Epoch [14/100] Running:
Epoch [14/100], Loss: 0.4643
Epoch [15/100] Running:
Epoch [15/100], Loss: 0.4612
Epoch [16/100] Running:
Epoch [16/100], Loss: 0.4518
Epoch [17/100] Running:
Epoch [17/100], Loss: 0.4494
Epoch [18/100] Running:
Epoch [18/100], Loss: 0.4481
Epoch [19/100] Running:
Epoch [19/100], Loss: 0.4461
Epoch [20/100] Running:
Epoch [20/100], Loss: 0.4448
Epoch [21/100] Running:
Epoch [21/100], Loss: 0.4397
Epoch [22/100] Running:
Epoch [22/100], Loss: 0.4387
Epoch [23/100] Running:
Epoch [23/100], Loss: 0.4380
Epoch [24/100] Running:
Epoch [24/100], Loss: 0.4371
Epoch [25/100] Running:
Epoch [25/100], Loss: 0.4368
Epoch [26/100] Running:
Epoch [26/100], Loss: 0.4340
Epoch [27/100] Running:
Epoch [27/100], Loss: 0.4332
Epoch [28/100] Running:
Epoch [28/100], Loss: 0.4330
Epoch [29/100] Running:
Epoch [29/100], Loss: 0.4325
Epoch [30/100] Running:
Epoch [30/100], Loss: 0.4320
Epoch [31/100] Running:
Epoch [31/100], Loss: 0.4308
Epoch [32/100] Running:
Epoch [32/100], Loss: 0.4305
Epoch [33/100] Running:
Epoch [33/100], Loss: 0.4303
Epoch [34/100] Running:
Epoch [34/100], Loss: 0.4301
Epoch [35/100] Running:
Epoch [35/100], Loss: 0.4299
Epoch [36/100] Running:
Epoch [36/100], Loss: 0.4292
Epoch [37/100] Running:
Epoch [37/100], Loss: 0.4290
Epoch [38/100] Running:
Epoch [38/100], Loss: 0.4290
Epoch [39/100] Running:
Epoch [39/100], Loss: 0.4288
Epoch [40/100] Running:
Epoch [40/100], Loss: 0.4288
Epoch [41/100] Running:
Epoch [41/100], Loss: 0.4283
Epoch [42/100] Running:
Epoch [42/100], Loss: 0.4283
Epoch [43/100] Running:
Epoch [43/100], Loss: 0.4282
Epoch [44/100] Running:
Epoch [44/100], Loss: 0.4282
Epoch [45/100] Running:
Epoch [45/100], Loss: 0.4282
Epoch [46/100] Running:
Epoch [46/100], Loss: 0.4279
Epoch [47/100] Running:
Epoch [47/100], Loss: 0.4279
Epoch [48/100] Running:
Epoch [48/100], Loss: 0.4279
Epoch [49/100] Running:
Epoch [49/100], Loss: 0.4279
Epoch [50/100] Running:
Epoch [50/100], Loss: 0.4278
Epoch [51/100] Running:
Epoch [51/100], Loss: 0.4277
Epoch [52/100] Running:
Epoch [52/100], Loss: 0.4277
Epoch [53/100] Running:
Epoch [53/100], Loss: 0.4277
Epoch [54/100] Running:
Epoch [54/100], Loss: 0.4277
Epoch [55/100] Running:
Epoch [55/100], Loss: 0.4277
Epoch [56/100] Running:
Epoch [56/100], Loss: 0.4276
Epoch [57/100] Running:
Epoch [57/100], Loss: 0.4276
Epoch [58/100] Running:
Epoch [58/100], Loss: 0.4276
Epoch [59/100] Running:
Epoch [59/100], Loss: 0.4276
Epoch [60/100] Running:
Epoch [60/100], Loss: 0.4276
Epoch [61/100] Running:
Epoch [61/100], Loss: 0.4276
Epoch [62/100] Running:
Epoch [62/100], Loss: 0.4275
Epoch [63/100] Running:
Epoch [63/100], Loss: 0.4275
Epoch [64/100] Running:
Epoch [64/100], Loss: 0.4275
Epoch [65/100] Running:
Epoch [65/100], Loss: 0.4275
Epoch [66/100] Running:
Epoch [66/100], Loss: 0.4275
Epoch [67/100] Running:
Epoch [67/100], Loss: 0.4276
Epoch [68/100] Running:
Epoch [68/100], Loss: 0.4276
Epoch [69/100] Running:
Epoch [69/100], Loss: 0.4275
Epoch [70/100] Running:
Epoch [70/100], Loss: 0.4275
Epoch [71/100] Running:
Epoch [71/100], Loss: 0.4275
Epoch [72/100] Running:
Epoch [72/100], Loss: 0.4275
Epoch [73/100] Running:
Epoch [73/100], Loss: 0.4275
Epoch [74/100] Running:
Epoch [74/100], Loss: 0.4275
Epoch [75/100] Running:
Epoch [75/100], Loss: 0.4275
Epoch [76/100] Running:
Epoch [76/100], Loss: 0.4275
Epoch [77/100] Running:
Epoch [77/100], Loss: 0.4276
Epoch [78/100] Running:
Epoch [78/100], Loss: 0.4275
Epoch [79/100] Running:
Epoch [79/100], Loss: 0.4275
Epoch [80/100] Running:
Epoch [80/100], Loss: 0.4275
Epoch [81/100] Running:
Epoch [81/100], Loss: 0.4275
Epoch [82/100] Running:
Epoch [82/100], Loss: 0.4275
Epoch [83/100] Running:
Epoch [83/100], Loss: 0.4275
Epoch [84/100] Running:
Epoch [84/100], Loss: 0.4274
Epoch [85/100] Running:
Epoch [85/100], Loss: 0.4276
Epoch [86/100] Running:
Epoch [86/100], Loss: 0.4275
Epoch [87/100] Running:
Epoch [87/100], Loss: 0.4275
Epoch [88/100] Running:
Epoch [88/100], Loss: 0.4275
Epoch [89/100] Running:
Epoch [89/100], Loss: 0.4275
Epoch [90/100] Running:
Epoch [90/100], Loss: 0.4275
Epoch [91/100] Running:
Epoch [91/100], Loss: 0.4275
Epoch [92/100] Running:
Epoch [92/100], Loss: 0.4275
Epoch [93/100] Running:
Epoch [93/100], Loss: 0.4276
Epoch [94/100] Running:
Epoch [94/100], Loss: 0.4275
Epoch [95/100] Running:
Epoch [95/100], Loss: 0.4275
Epoch [96/100] Running:
Epoch [96/100], Loss: 0.4275
Epoch [97/100] Running:
Epoch [97/100], Loss: 0.4275
Epoch [98/100] Running:
Epoch [98/100], Loss: 0.4275
Epoch [99/100] Running:
Epoch [99/100], Loss: 0.4275
Epoch [100/100] Running:
Epoch [100/100], Loss: 0.4275
Training complete!
