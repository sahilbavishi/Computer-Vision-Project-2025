# -*- coding: utf-8 -*-
"""CVis - coursework_working_rn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13_-gMOVvZT8HbyUhMmRN8M-_wm17ZgkJ

# Computer Vision Coursework
## Image segmentation

Intro...
"""

# Setting code-behaviour variables
genVISUALS = False  # set to False in order to avoid time-consuming visualizations' genaration (images are instead displayed as pre-saved in 'Output/Visuals' folder)
dpi = 500           # dpi for .pdf-saved images visualization (with genVISUALS = False)
rePREPROC = False   # if True, the input images' resizing and augmentation are run, otherwise the saved outcomes are used
random_seed = 42

# Importing useful libraries
# %pip install pdf2image
#conda install poppler-utils
import os
from pathlib import Path
from pdf2image import convert_from_path # (also install !apt-get install poppler-utils)
from IPython.display import display
import numpy as np
from PIL import Image
import seaborn as sns
import cv2
import tensorflow as tf
import albumentations as A
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import torch.nn.functional as F
# These are libraries for Model Creation:
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from torch.optim.lr_scheduler import StepLR

# # Mounting Drive
# if os.path.ismount('/content/drive'):
#   drive.flush_and_unmount()
# drive.mount('/content/drive')

# # Define both user paths
# path_1 = "/content/drive/Othercomputers/Il mio laptop/Matteo/Magistrale/Un. of Edinburgh/Corsi/CVis/Coursework"
# path_2 = "/content/drive/MyDrive/Coursework"

# # Move to the correct directory based on the user path
# if Path(path_1).exists():
#     %cd "{path_1}"
# else:
#     %cd "{path_2}"
# %ls

# Loading input data
input_folder_trainval = 'Data/Input/TrainVal'
input_trainval = [f for f in os.listdir(input_folder_trainval+'/color') if f.lower().endswith(('.jpg', '.jpeg'))]
input_trainval_labels = [f for f in os.listdir(input_folder_trainval+'/label') if f.lower().endswith(('.png'))]

input_folder_test = 'Data/Input/Test'
input_test = [f for f in os.listdir(input_folder_test+'/color') if f.lower().endswith(('.jpg', '.jpeg'))]
input_test_labels = [f for f in os.listdir(input_folder_test+'/label') if f.lower().endswith(('.png'))]

output_folder = 'Data/Output'
output_folder_resized = os.path.join(output_folder, 'Resized')
output_folder_augmented = os.path.join(output_folder, 'Augmented')

output_folder_resized_color = os.path.join(output_folder_resized, 'color')
output_folder_resized_label = os.path.join(output_folder_resized, 'label')
output_folder_augmented_color = os.path.join(output_folder_augmented, 'color')
output_folder_augmented_label = os.path.join(output_folder_augmented, 'label')

"""### 1. Dataset preprocessing and augmentation

The images are resized to the dimensions (H<sub>min</sub>, W<sub>min</sub>), thus to take the Q3 height and width size over all the images in the dataset; the instances will be processed in this format, then the output resized back to the original dimensions...

Furthermore...(data augmentation)
"""

# Gauging image sizes
if genVISUALS or rePREPROC:
  widths = []
  heights = []
  for img_file in input_trainval:
      with Image.open(os.path.join(input_folder_trainval+'/color', img_file)) as img:
          width, height = img.size
          widths.append(width)
          heights.append(height)

# Visualizing histogram distribution of dimensions
if genVISUALS or rePREPROC:
  plt.figure(figsize=(12, 5))
  plt.subplot(1, 2, 1)
  plt.hist(widths, bins=20, color='lightblue', edgecolor='black')
  plt.xlabel("Width")
  plt.ylabel("Frequency")
  plt.title("Widths distribution")
  plt.subplot(1, 2, 2)
  plt.hist(heights, bins=20, color='skyblue', edgecolor='black')
  plt.xlabel("Height")
  plt.ylabel("Frequency")
  plt.title("Heights distribution")
  plt.tight_layout()
  plt.savefig(Path('Data/Output/Visuals/dimensions_histogram.pdf'))
  plt.show()
else:
  for image in convert_from_path(Path('Data/Output/Visuals/dimensions_histogram.pdf'), dpi=dpi):
    display(image)

# Visualizing boxplot distribution of dimensions
if genVISUALS or rePREPROC:
  plt.figure(figsize=(10, 6))
  data = {"Width": widths, "Height": heights}
  sns.boxplot(data=data, palette=['lightblue', 'skyblue'])
  plt.title("Boxplot of Widths and Heights")
  plt.ylabel("Pixels")
  plt.tight_layout()
  plt.savefig(Path('Data/Output/Visuals/dimensions_boxplot.pdf'))
  plt.show()
else:
  for image in convert_from_path(Path('Data/Output/Visuals/dimensions_boxplot.pdf'), dpi=dpi):
    display(image)

# Printing stats
print(f"Input set size: {len(input_trainval)}\n")
if rePREPROC:
  min_width, min_height = np.min(widths), np.min(heights)
  median_width, median_height = np.median(widths), np.median(heights)
  mean_width, mean_height = np.mean(widths), np.mean(heights)
  mode_width, mode_height = max(set(widths), key=widths.count), max(set(heights), key=heights.count)
  q3_width, q3_height = np.percentile(widths, 75), np.percentile(heights, 75)
  iqr_width = np.percentile(widths, 75) - np.percentile(widths, 25)
  iqr_height = np.percentile(heights, 75) - np.percentile(heights, 25)
  outlier_count_width = np.sum(widths > (q3_width + 1.5 * iqr_width))
  outlier_count_height = np.sum(heights > (q3_height + 1.5 * iqr_height))
  print(f"Min Size: {min_width}x{min_height}")
  print(f"Median Size: {median_width}x{median_height}")
  print(f"Mean Size: {mean_width:.2f}x{mean_height:.2f}")
  print(f"Mode Size: {mode_width}x{mode_height}")
  print(f"Q3 Size: {q3_width}x{q3_height}")
  print(f"Outliers in width: {outlier_count_width}")
  print(f"Outliers in height: {outlier_count_height}")
else:
  print("Q3 width and height values (both 500 pixels) were chosen for resizing.")

"""#### a) Resizing"""

# Resizing images (to Q3 width, Q3 height)
if rePREPROC:
  imgResize = (int(q3_width), int(q3_height))
  widthsNP = np.array(widths)
  heightsNP = np.array(heights)
  i = 0
  for img_file in input_trainval:
      with Image.open(os.path.join(input_folder_trainval+'/color', img_file)) as img:
          img_resized = img.resize(imgResize, Image.Resampling.LANCZOS)
          if img_resized.mode == "RGBA":
            img_resized = img_resized.convert("RGB")
          img_resized.save(os.path.join(output_folder_resized_color, img_file), format="JPEG")
          i += 1
  print(f"{i} images resized to {int(q3_width)}x{int(q3_height)} and saved in {output_folder_resized_color}")

# Resizing labels
  imgResize = (int(q3_width), int(q3_height))
  widthsNP = np.array(widths)
  heightsNP = np.array(heights)
  i = 0
  for img_file in input_trainval_labels:
      with Image.open(os.path.join(input_folder_trainval+'/label', img_file)) as img:
          img_resized = img.resize(imgResize, Image.Resampling.LANCZOS)
          if img_resized.mode == "RGBA":
            img_resized = img_resized.convert("RGB")
          img_resized.save(os.path.join(output_folder_resized_label, img_file), format="PNG")
          i += 1
  print(f"{i} labels resized to {int(q3_width)}x{int(q3_height)} and saved in {output_folder_resized_label}")

else:
  print("Using previously resized images and labels (500x500).")
  imgResize = (500, 500)

"""#### b) Augmenting dataset"""

# # b) Augmenting dataset (inspired to U-Net)
# if rePREPROC:
#   augmentation = A.Compose([
#       A.HorizontalFlip(p=0.5),
#       A.VerticalFlip(p=0.5),
#       A.RandomRotate90(p=0.5),
#       A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),
#       A.RandomBrightnessContrast(p=0.2),
#       A.GaussianBlur(blur_limit=(3, 7), p=0.2),
#       A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),
#   ])

#   i = 0
#   for img_file in os.listdir(output_folder_resized_color):
#       img_path = os.path.join(output_folder_resized_color, img_file)
#       img = cv2.imread(img_path)
#       if img is None:
#           continue
#       img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # converting BGR to RGB
#       augmented = augmentation(image=img)['image']
#       output_path = os.path.join(output_folder_augmented_color, f'aug_{img_file}')
#       Image.fromarray(augmented).save(output_path)
#       i += 1

#   print(f"{i} images augmented, output saved in {output_folder_augmented_color}")
# else:
#   print("Using previously augmented data.")

# b) Augmenting dataset (inspired by U-Net)

augmentation = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5, always_apply=True)  # Ensure it's always applied
], additional_targets={'mask': 'mask'})  # Use 'mask' instead of 'image' to avoid interpolation issues

if rePREPROC:
    i = 0
    for img_file in os.listdir(output_folder_resized_color):
        img_path = os.path.join(output_folder_resized_color, img_file)
        mask_filename = os.path.splitext(img_file)[0] + ".png"  # Replace .jpeg with .png
        mask_path = os.path.join(output_folder_resized_label, mask_filename)

        # Load image and mask
        img = cv2.imread(img_path)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask in grayscale

        if img is None or mask is None:
            print(f"Skipping {img_file} as corresponding image or mask is missing.")
            continue

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB

        # Apply augmentation
        augmented = augmentation(image=img, mask=mask)
        aug_img, aug_mask = augmented['image'], augmented['mask']

        # Ensure mask remains categorical by reapplying np.uint8
        aug_mask = np.round(aug_mask).astype(np.uint8)

        # Ensure mask retains only valid values
        unique_values = np.unique(mask)
        for value in np.unique(aug_mask):
            if value not in unique_values:
                aug_mask[aug_mask == value] = 0  # Convert any unexpected values to 0

        # Save augmented image
        output_img_path = os.path.join(output_folder_augmented_color, f'aug_{img_file}')
        Image.fromarray(aug_img).save(output_img_path, "JPEG")  # Save as JPEG

        # Save augmented mask
        output_mask_filename = f'aug_{mask_filename}'  # Ensure naming consistency
        output_mask_path = os.path.join(output_folder_augmented_label, output_mask_filename)
        Image.fromarray(aug_mask).save(output_mask_path, "PNG")  # Save as PNG

        i += 1

    print(f"{i} images and masks augmented, output saved in {output_folder_augmented_color} & {output_folder_augmented_label}")


    # Number of images needed to balance cats and dogs
    target_cat_count = 2492
    existing_cat_count = 1188
    additional_cats_needed = target_cat_count - existing_cat_count

    def is_cat(filename):
        cat_breeds = ["abyssinian", "bengal", "birman", "bombay", "british_shorthair", 
                    "egyptian_mau", "maine_coon", "persian", "ragdoll", "russian_blue", 
                    "siamese", "sphynx"]
        
        filename_lower = filename.lower()  # Convert to lowercase for case-insensitive search
        return any(breed in filename_lower for breed in cat_breeds)

    # Calculate how many augmentations per cat image
    cat_images = [img for img in os.listdir(output_folder_augmented_color) if is_cat(img.lower())]
    augmentations_per_image = 1  # Distribute augmentations
    # print(augmentations_per_image,additional_cats_needed,len(cat_images))

    i = 0
    for img_file in cat_images:
        img_path = os.path.join(output_folder_augmented_color, img_file)
        mask_filename = os.path.splitext(img_file)[0] + ".png"
        mask_path = os.path.join(output_folder_augmented_label, mask_filename)

        img = cv2.imread(img_path)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        if img is None or mask is None:
            print(f"Skipping {img_file} as image or mask is missing.")
            continue

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Apply augmentation multiple times to balance dataset
        for j in range(augmentations_per_image):
            augmented = augmentation(image=img, mask=mask)
            aug_img, aug_mask = augmented['image'], augmented['mask']

            # Ensure mask remains categorical
            aug_mask = np.round(aug_mask).astype(np.uint8)
            unique_values = np.unique(mask)
            for value in np.unique(aug_mask):
                if value not in unique_values:
                    aug_mask[aug_mask == value] = 0  

            # Save augmented image
            output_img_path = os.path.join(output_folder_augmented_color, f'aug_{i}_{img_file}')
            Image.fromarray(aug_img).save(output_img_path, "JPEG")

            # Save augmented mask
            output_mask_filename = f'aug_{i}_{mask_filename}'
            output_mask_path = os.path.join(output_folder_augmented_label, output_mask_filename)
            Image.fromarray(aug_mask).save(output_mask_path, "PNG")

            i += 1
            if i >= additional_cats_needed:
                break  # Stop when we reach the target count

else:
    print("Using previously augmented data.")


"""#### c) Preparing datasets"""

# Preparing train, valid and test sets
validSize = 0.2
batchSize = 4
imgChannels = 3 ## this was 3
inputSize = (imgResize[0], imgResize[1], imgChannels)

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Define preprocessed size
preprocessedSize = (256, 256)  # Desired size for input images and masks
inputSize = (256,256,3)

trainval_images = [os.path.join(output_folder_augmented_color, f) for f in os.listdir(output_folder_augmented_color) if f.endswith('.jpg')]
trainval_masks = [os.path.join(output_folder_augmented_label, f) for f in os.listdir(output_folder_augmented_label) if f.endswith('.png')]
train_images, val_images, train_masks, val_masks = train_test_split(trainval_images, trainval_masks, test_size=validSize, random_state=random_seed)

test_images = [os.path.join(input_folder_test, 'color', f) for f in os.listdir(os.path.join(input_folder_test, 'color')) if f.endswith('.jpg')]
test_masks = [os.path.join(input_folder_test, 'label', f) for f in os.listdir(os.path.join(input_folder_test, 'label')) if f.endswith('.png')]


def preprocess_image(image_path, target_size=preprocessedSize):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=imgChannels)
    image = tf.image.resize(image, target_size)  # Resize to preprocessed size
    image = image / 255.0  # Normalizing to [0, 1]
    return image

def preprocess_mask(mask_path, target_size=preprocessedSize):
    mask_path = tf.strings.join(['', mask_path])  # Converting to string if tensor
    mask_extension = tf.strings.split(mask_path, '.')[-1]  # Handling .jpg as grayscale or .png
    mask_extension = tf.strings.lower(mask_extension)
    mask = tf.io.read_file(mask_path)
    if mask_extension == 'jpg' or mask_extension == 'jpeg':
        mask = tf.image.decode_jpeg(mask, channels=4)
    else:
        mask = tf.image.decode_png(mask, channels=4)
    mask = tf.image.resize(mask, target_size, method='nearest')  # Resize to preprocessed size
    mask = tf.squeeze(mask)
    mask = tf.cast(mask, tf.int32)
    return mask

def create_dataset(image_paths, mask_paths, batch_size=batchSize, shuffle=True):
    def load_and_preprocess(image_path, mask_path):
        image = preprocess_image(image_path)
        mask = preprocess_mask(mask_path)
        return image, mask
    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))
    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    if shuffle:
        dataset = dataset.shuffle(buffer_size=len(image_paths))
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset


def preprocess_label(label):
    label = tf.expand_dims(label, axis=-1)  # Convert (500, 500) → (500, 500, 1)
    return tf.cast(label, tf.int32)  # Ensure labels are integers


train_dataset = create_dataset(train_images, train_masks, batch_size=batchSize, shuffle=True)
val_dataset = create_dataset(val_images, val_masks, batch_size=batchSize, shuffle=False)
test_dataset = create_dataset(test_images, test_masks, batch_size=batchSize, shuffle=False)

# Print dataset sizes and input dimensions
print(f"Train set size: {len(train_images)} ({(1-validSize)*100}%)")
print(f"Valid set size: {len(val_images)} ({(validSize)*100}%)")
print(f" Test set size: {len(test_images)}")
print(f"\nInput dimension: {preprocessedSize + (imgChannels,)}")  # Adjusted for the preprocessed size
print(f"Batches' size: {batchSize}")

"""

def preprocess_image(image_path, target_size=imgResize):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=imgChannels)
    image = tf.image.resize(image, target_size)
    image = image / 255.0 # normalizing to [0, 1]
    return image

def preprocess_mask(mask_path, target_size=imgResize):
    mask_path = tf.strings.join(['', mask_path]) # converting to string if tensor
    mask_extension = tf.strings.split(mask_path, '.')[-1] # handling .jpg as grayscale or .png
    mask_extension = tf.strings.lower(mask_extension)
    mask = tf.io.read_file(mask_path)
    if mask_extension == 'jpg' or mask_extension == 'jpeg':
        mask = tf.image.decode_jpeg(mask, channels=3)
    else:
        mask = tf.image.decode_png(mask, channels=3)
    mask = tf.image.resize(mask, target_size, method='nearest')
    mask = tf.squeeze(mask)
    mask = tf.cast(mask, tf.int32)
    return mask


train_dataset = create_dataset(train_images, train_masks, batch_size=batchSize, shuffle=True)
val_dataset = create_dataset(val_images, val_masks, batch_size=batchSize, shuffle=False)
test_dataset = create_dataset(test_images, test_masks, batch_size=batchSize, shuffle=False)

#train_dataset = train_dataset.map(lambda x, y: (x, preprocess_label(y)))
#val_dataset = val_dataset.map(lambda x, y: (x, preprocess_label(y)))
#test_dataset = test_dataset.map(lambda x, y: (x, preprocess_label(y)))


print(f"Train set size: {len(train_images)} ({(1-validSize)*100}%)")
print(f"Valid set size: {len(val_images)} ({(validSize)*100}%)")
print(f" Test set size: {len(test_images)}")
print(f"\nInput dimension: {inputSize}")
print(f"Batches' size: {batchSize}")
"""


# Preparing train, valid and test sets

# NOW USING NOT AUGMENTED INPUT FOR TRAINING: _____
#image_folder = input_folder_trainval + '/color'  # -> output_folder_augmented
#mask_folder = input_folder_trainval + '/label'   # TODO: augmented masks folder
# - - - - - - - - - - - - - - - - - - - - - - - - -

# validSize = 0.2
# batchSize = 16
# imgChannels = 3
# inputSize = (imgResize[0], imgResize[1], imgChannels)

# trainval_images = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.jpg')])
# trainval_masks = sorted([os.path.join(mask_folder, f.replace('.jpg', '.png')) for f in os.listdir(image_folder) if f.endswith('.jpg')])
# train_images, val_images, train_masks, val_masks = train_test_split(trainval_images, trainval_masks, test_size=validSize, random_state=random_seed)

# test_images = sorted([os.path.join(input_folder_test, 'color', f) for f in os.listdir(os.path.join(input_folder_test, 'color')) if f.endswith('.jpg')])
# test_masks = sorted([os.path.join(input_folder_test, 'label', f.replace('.jpg', '.png')) for f in os.listdir(os.path.join(input_folder_test, 'label')) if f.endswith('.png')])

# # Converting to TensorFlow datasets
# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_masks))
# val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_masks))
# test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_masks))

# def load_image_and_mask(image_path, mask_path):
#     image = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=imgChannels)
#     image = tf.image.resize(image, imgResize)
#     image = image / 255.0 # normalizing to [0, 1]
#     mask = tf.image.decode_png(tf.io.read_file(mask_path), channels=1)
#     mask = tf.image.resize(mask, imgResize, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
#     return image, mask

# # Applying resizing and normalization
# train_dataset = train_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)
# val_dataset = val_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)
# test_dataset = test_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)

# # Prefetching
# train_dataset = train_dataset.batch(batchSize).prefetch(tf.data.AUTOTUNE)
# val_dataset = val_dataset.batch(batchSize).prefetch(tf.data.AUTOTUNE)
# test_dataset = test_dataset.batch(batchSize).prefetch(tf.data.AUTOTUNE)

# Preparing sample test image
sampleImg = Image.open("Data/Output/Resized/color/Birman_159.jpg").convert('RGB')
sampleLabel = Image.open("Data/Output/Resized/label/Birman_159.png").convert('L') # (grayscale)

#for image, label in train_dataset.take(1):
#    print("Image shape:", image.shape)
#    print("Label shape:", label.shape)

"""### 2. Segmentation network

Description...

#### a) UNet-based end-to-end NN (TO FIX)
"""

import gc
import tensorflow as tf
gc.collect()
tf.keras.backend.clear_session()

# Building and training the model

classesNum = 4 # number of output classes
epochsNum = 2 # number of training epochs
batchSize = 16

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pickle as pkl

# Define the UNet architecture
class UNet(nn.Module):
    def __init__(self, num_classes=4):
        super(UNet, self).__init__()

        # Encoder
        self.enc1 = self.conv_block(3, 64)
        self.pool1 = nn.MaxPool2d(2)

        self.enc2 = self.conv_block(64, 128)
        self.pool2 = nn.MaxPool2d(2)

        self.enc3 = self.conv_block(128, 256)
        self.pool3 = nn.MaxPool2d(2)

        self.enc4 = self.conv_block(256, 512)

        # Decoder
        self.up5 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec5 = self.conv_block(512, 256)

        self.up6 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec6 = self.conv_block(256, 128)

        self.up7 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec7 = self.conv_block(128, 64)

        # Final output layer
        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)

    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        # Encoder
        c1 = self.enc1(x)
        p1 = self.pool1(c1)

        c2 = self.enc2(p1)
        p2 = self.pool2(c2)

        c3 = self.enc3(p2)
        p3 = self.pool3(c3)

        c4 = self.enc4(p3)

        # Decoder
        u5 = self.up5(c4)
        u5 = torch.cat([u5, c3], dim=1)
        c5 = self.dec5(u5)

        u6 = self.up6(c5)
        u6 = torch.cat([u6, c2], dim=1)
        c6 = self.dec6(u6)

        u7 = self.up7(c6)
        u7 = torch.cat([u7, c1], dim=1)
        c7 = self.dec7(u7)

        outputs = self.final_conv(c7)

        return outputs

# Define hyperparameters
epochs = 100 
batch_size = 16
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize model, optimizer, and loss function
model = UNet(num_classes=4).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()


# Evaluation
def evaluate(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, masks in test_loader:
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            predictions = torch.argmax(outputs, dim=1)
            correct += (predictions == masks).sum().item()
            total += masks.numel()

    accuracy = correct / total
    print(f"Test Accuracy: {accuracy:.4f}")

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import torch

class UNetDataset(Dataset):
    def __init__(self, image_files, mask_files, transform=None, target_transform=None, max_images=100):
        self.image_files = image_files  # Limit to first 100 images
        self.mask_files = mask_files  # Limit to first 100 masks
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        label_path = self.image_files[idx].replace('.jpg', '.png').replace('color', 'label')
        image = Image.open(img_path).convert('RGB')
        label = Image.open(label_path).convert('L')  # Load label as grayscale

        if self.transform:
            image = self.transform(image)
            
        label = label.resize((256, 256), Image.NEAREST)

        # Convert label to tensor
        label = torch.tensor(np.array(label), dtype=torch.long)  # Shape: (H, W)

        # # Map pixel values to class indices
        label = torch.where(label == 38, 1, label)
        label = torch.where(label == 75, 2, label)
        label = torch.where(label == 255, 3, label)
        label = torch.where(label == 0, 0, label)  # Ensure 0 stays as class 0

        # mapping = {38: 1, 75: 2, 255: 0, 0: 0}
        # label = torch.tensor(np.vectorize(mapping.get)(np.array(label)), dtype=torch.long)

        

        return image, label  # No one-hot encoding

# Define transformations
image_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor()
])



# Load dataset with first 100 images
train_dataset = UNetDataset(train_images, train_masks, transform=image_transform, target_transform=None, max_images=100)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

val_dataset = UNetDataset(val_images, val_masks, transform=image_transform, target_transform=None, max_images=100)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

test_dataset = UNetDataset(test_images, test_masks, transform=image_transform, target_transform=None, max_images=100)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import pickle as pkl

# Define the training function
def train_unet(model, train_loader, val_loader, epochs=2, model_save_path="/home/s2677266/CVis/Data/Output/Models/", device="cuda" if torch.cuda.is_available() else "cpu"):
    model.to(device)
    criterion = nn.CrossEntropyLoss()  # For multi-class segmentation
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        for images, masks in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            images, masks = images.to(device), masks.long().squeeze(1).to(device)  # Ensure correct shape
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss / len(train_loader):.4f}")

        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for images, masks in val_loader:
                images, masks = images.to(device), masks.long().squeeze(1).to(device)
                outputs = model(images)
                loss = criterion(outputs, masks)
                val_loss += loss.item()
        
        print(f"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss / len(val_loader):.4f}")

        # Save model after each epoch
        epoch_model_path = f"{model_save_path}unet_model_epoch_{epoch+1}.pth"
        torch.save(model.state_dict(), epoch_model_path)
        print(f"Model saved at {epoch_model_path}")

    print("Training complete!")

# Define the testing function
def test_unet(model, test_loader, device="cuda" if torch.cuda.is_available() else "cpu"):
    model.to(device)
    model.eval()
    test_loss = 0
    criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for images, masks in test_loader:
            images, masks = images.to(device), masks.long().squeeze(1).to(device)
            outputs = model(images)
            loss = criterion(outputs, masks)
            test_loss += loss.item()

    print(f"Test Loss: {test_loss / len(test_loader):.4f}")

# Call the training function
train_unet(model, train_loader, val_loader, epochs=100)

# Evaluate on the test set
test_unet(model, test_loader)

























'''
def crop_and_concat(target, reference):
    """Ensures target tensor matches the shape of reference tensor before concatenation."""
    target_shape = K.int_shape(target)
    reference_shape = K.int_shape(reference)

    if target_shape is None or reference_shape is None:
        raise ValueError("Input tensors must have defined shapes.")

    height_diff = target_shape[1] - reference_shape[1]
    width_diff = target_shape[2] - reference_shape[2]

    # Ensure valid cropping values (avoid negative numbers)
    crop_h = (max(height_diff // 2, 0), max(height_diff - (height_diff // 2), 0))
    crop_w = (max(width_diff // 2, 0), max(width_diff - (width_diff // 2), 0))

    # Apply cropping only if needed
    if crop_h != (0, 0) or crop_w != (0, 0):
        target = layers.Cropping2D(cropping=(crop_h, crop_w))(target)

    # Resize if still mismatched (Failsafe)
    target = layers.Resizing(reference_shape[1], reference_shape[2])(target)

    return layers.Concatenate()([target, reference])


def unet_model(input_size=inputSize, num_classes=classesNum):
    inputs = layers.Input(input_size)

    # Encoder
    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)
    p1 = layers.MaxPooling2D((2, 2))(c1)

    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)
    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)
    p2 = layers.MaxPooling2D((2, 2))(c2)

    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)
    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)
    p3 = layers.MaxPooling2D((2, 2))(c3)

    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)
    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)

    # Decoder
    u5 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c4)
    u5 = crop_and_concat(u5, c3)
    c5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u5)
    c5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c5)

    u6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)
    u6 = crop_and_concat(u6, c2)
    c6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u6)
    c6 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c6)

    u7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)
    u7 = crop_and_concat(u7, c1)
    c7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u7)
    c7 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c7)

    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(c7)

    model = models.Model(inputs, outputs)
    return model

# Create and compile the model
model = unet_model(input_size=inputSize, num_classes=classesNum)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Training
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=epochsNum,
    batch_size=batchSize
)
with open('Data/Output/Models/unet_history.pkl', 'wb') as f:
    pkl.dump(history, f)
with open('Data/Output/Models/unet_model.pkl', 'wb') as f:
    pkl.dump(model, f)

# Evaluation
test_loss, test_accuracy = model.evaluate(test_dataset)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

# Testing the model (TO FIX)
image = preprocess_image('path_to_image.jpg')
mask = preprocess_mask('path_to_mask.png')
# Visualizing predictions
def visualize_prediction(image, mask, prediction):
    plt.figure(figsize=(15, 5))

    # Original Image
    plt.subplot(1, 3, 1)
    plt.imshow(image)
    plt.title('Original Image')

    # Ground Truth Mask
    plt.subplot(1, 3, 2)
    plt.imshow(mask, cmap='jet')
    plt.title('Ground Truth Mask')

    # Predicted Mask
    plt.subplot(1, 3, 3)
    plt.imshow(np.argmax(prediction, axis=-1), cmap='jet')
    plt.title('Predicted Mask')

    plt.show()

image, mask = next(iter(test_dataset))
prediction = model.predict(tf.expand_dims(image, axis=0))
visualize_prediction(image, mask, prediction[0])
'''
"""#### b) Autoencoders (TO FIX)"""

# # This is the encoder part that learns on raw images and no labels

# # Define the Autoencoder
# class Autoencoder(nn.Module):
#     def __init__(self):
#         super(Autoencoder, self).__init__()
#         # Encoder
#         self.encoder = nn.Sequential(
#             nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(kernel_size=2, stride=2),  # 500 -> 250
#             nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(kernel_size=2, stride=2),  # 250 -> 125
#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#         )
#         # Decoder
#         self.decoder = nn.Sequential(
#             nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),   # 125 -> 250
#             nn.ReLU(),
#             nn.ConvTranspose2d(64, 3, kernel_size=2, stride=2),     # 250 -> 500
#         )

#     def forward(self, x):
#         x = self.encoder(x)
#         x = self.decoder(x)
#         return x

# # Custom Dataset for loading images
# class ImageDataset(Dataset):
#     def __init__(self, image_files, transform=None, max_images=100):
#         #self.image_folder = image_folder
#         #self.image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
#         self.image_files = image_files # Limit to first 100 images
#         self.transform = transform

#     def __len__(self):
#         return len(self.image_files)

#     def __getitem__(self, idx):
#         img_path = self.image_files[idx]
#         image = Image.open(img_path).convert('RGB')
#         if self.transform:
#             image = self.transform(image)
#         return image

# # Transformations
# transform = transforms.Compose([
#     transforms.Resize((500, 500)),  # Resize images to 500x500
#     transforms.ToTensor()           # Convert to tensor
# ])

# # Load dataset with first 100 images
# dataset = ImageDataset(image_files=train_images, transform=transform, max_images=100)
# dataloader = DataLoader(dataset, batch_size=batchSize, shuffle=True)  # Reduce batch size to avoid memory issues

# # Initialize model, loss, and optimizer
# autoencoder = Autoencoder()
# criterion = nn.MSELoss()  # Mean Squared Error for reconstruction
# optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)

# # Training the Autoencoder
# num_epochs = 3
# for epoch in range(num_epochs):
#     print(f"Epoch [{epoch+1}/{num_epochs}] Running:")
#     for batch in dataloader:
#         optimizer.zero_grad()
#         outputs = autoencoder(batch)
#         loss = criterion(outputs, batch)
#         loss.backward()
#         optimizer.step()

#     print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

#     # Save the model after each epoch
#     torch.save(autoencoder.state_dict(), f'Data/Output/Models/autoencoder_epoch_{epoch+1}.pth')



# WORKING PART B



import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os

from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()


# Define the Autoencoder
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
        nn.LeakyReLU(0.2),
        nn.MaxPool2d(kernel_size=2, stride=2),  # 500 -> 250
        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
        nn.LeakyReLU(0.2),
        nn.MaxPool2d(kernel_size=2, stride=2),  # 250 -> 125
        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
        nn.LeakyReLU(0.2),
        nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),  # Add another layer for a richer encoding
        nn.LeakyReLU(0.2),
    )

        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.LeakyReLU(0.2),
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),
            nn.Sigmoid()
        )


    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x
    
    
  # Below is the code to train the autoencoder  
    """
    
    
'''
# # Custom Dataset for loading images
class ImageDataset(Dataset):
    def __init__(self, image_files, transform=None, max_images=100):
        self.image_files = image_files  # Limit to first 100 images
        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        return image

# Transformations
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1,1]
])


# Load dataset with first 100 images
dataset = ImageDataset(image_files=train_images, transform=transform, max_images=100)
dataloader = DataLoader(dataset, batch_size=batchSize, shuffle=True)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# Initialize model, loss, and optimizer
autoencoder = Autoencoder().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3) 

def weights_init(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
        nn.init.xavier_uniform_(m.weight)  # Xavier initialization
        if m.bias is not None:
            nn.init.zeros_(m.bias)

autoencoder.apply(weights_init)

# Training the Autoencoder
num_epochs = 30
for epoch in range(num_epochs):
    print(f"Epoch [{epoch+1}/{num_epochs}] Running:")
    torch.cuda.empty_cache()
    for batch in dataloader:
        batch = batch.to(device)
        optimizer.zero_grad()

        with autocast():  # Use autocast only here
            outputs = autoencoder(batch)
            loss = criterion(outputs, batch)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

    # Save the model after each epoch
    torch.save(autoencoder.state_dict(), f'Data/Output/Models/updated_working_autoencoder_epoch_{epoch+1}.pth')
'''


# This is the decoder part that learns with the fixed encoder above
batchSize = 16

'''
# Segmentation Decoder Model
class SegmentationDecoder(nn.Module):
    def __init__(self, encoder):
        super(SegmentationDecoder, self).__init__()
        self.encoder = encoder
        for param in self.encoder.parameters():
            param.requires_grad = False  # Freeze encoder
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            
            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            
            nn.Conv2d(32, 4, kernel_size=1)  # Final output layer (logits)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x  # Logits (raw scores before softmax)


# Dice Loss Function
def dice_loss(pred, target, smooth=1.):
    pred = torch.softmax(pred, dim=1)  # Convert logits to probabilities
    target_one_hot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2)  # (B, C, H, W)
    
    intersection = (pred * target_one_hot).sum(dim=(2, 3))
    union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))
    
    dice = 1 - (2. * intersection + smooth) / (union + smooth)
    return dice.mean()  # Mean dice loss over batch


# Combined Loss Function
class CombinedLoss(nn.Module):
    def __init__(self, ce_weight=1.0, dice_weight=1.0):
        super().__init__()
        self.ce = nn.CrossEntropyLoss()
        self.dice_weight = dice_weight
        self.ce_weight = ce_weight

    def forward(self, pred, target):
        ce = self.ce(pred, target)
        dice = dice_loss(pred, target)
        return self.ce_weight * ce + self.dice_weight * dice


# Custom Dataset for loading images and labels
class SegmentationDataset(Dataset):
    def __init__(self, image_files, label_files, transform=None, max_images=100):
        self.image_files = image_files
        self.label_files = label_files
        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        label_path = self.image_files[idx].replace('.jpg', '.png').replace('color', 'label')

        image = Image.open(img_path).convert('RGB')
        label = Image.open(label_path).convert('L')  # Load label as grayscale

        if self.transform:
            image = self.transform(image)
            
        label = label.resize((256, 256), Image.NEAREST)

        # Convert label to tensor
        label = torch.tensor(np.array(label), dtype=torch.long)  # Shape: (H, W)

        # Map pixel values to class indices
        label = torch.where(label == 38, 1, label)
        label = torch.where(label == 75, 2, label)
        label = torch.where(label == 255, 3, label)
        label = torch.where(label == 0, 0, label)  # Ensure 0 stays as class 0

        return image, label  # No one-hot encoding


# Transformations
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
])


# Load segmentation dataset with first 100 images
segmentation_dataset = SegmentationDataset(
    image_files=train_images,
    label_files=train_masks,
    transform=transform,
    max_images=100
)
segmentation_dataloader = DataLoader(segmentation_dataset, batch_size=batchSize, shuffle=True)


# Load the pre-trained autoencoder
autoencoder = Autoencoder()
autoencoder.load_state_dict(torch.load('Data/Output/Models/updated_working_autoencoder_epoch_30.pth'))

# Initialize the segmentation model
segmentation_model = SegmentationDecoder(autoencoder.encoder)

# Define loss and optimizer for segmentation
# segmentation_criterion = CombinedLoss(ce_weight=1.0, dice_weight=1.0)  # Use CombinedLoss
segmentation_criterion = nn.CrossEntropyLoss()#(weight=class_weights)
segmentation_optimizer = optim.Adam(segmentation_model.decoder.parameters(), lr=1e-3)

# Training the Segmentation Model

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
segmentation_model.to(device)
segmentation_criterion.to(device)  # Move loss function to GPU

scheduler = torch.optim.lr_scheduler.StepLR(segmentation_optimizer, step_size=3, gamma=0.5)
'''

"""



# Define the Segmentation Decoder
# class SegmentationDecoder(nn.Module):
#     def __init__(self, encoder):
#         super(SegmentationDecoder, self).__init__()
#         self.encoder = encoder
#         for param in self.encoder.parameters():
#             param.requires_grad = False  # Freeze encoder
        
#         self.decoder = nn.Sequential(
#             nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
#             nn.BatchNorm2d(128),
#             nn.ReLU(),
#             nn.Dropout(0.2),

#             nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
#             nn.BatchNorm2d(64),
#             nn.ReLU(),
#             nn.Dropout(0.2),

#             nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),
#             nn.BatchNorm2d(32),
#             nn.ReLU(),

#             nn.Conv2d(32, 3, kernel_size=1)  # Final output layer (logits)
#         )

#     def forward(self, x):
#         x = self.encoder(x)
#         x = self.decoder(x)
#         return x  # Direct logits output

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Residual Block
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        residual = x
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        x += residual  # Skip connection
        return self.relu(x)

# Enhanced Segmentation Decoder
class SegmentationDecoder(nn.Module):
    def __init__(self, encoder):
        super(SegmentationDecoder, self).__init__()
        
        # Partially freeze encoder (freeze first few layers, fine-tune deeper ones)
        for param in list(encoder.parameters())[:4]:  # Adjust layers to freeze
            param.requires_grad = False
        
        self.encoder = encoder

        # Decoder with residual blocks and skip connections
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            ResidualBlock(128, 128),
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            ResidualBlock(64, 64),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 3, kernel_size=1)  # Output logits
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x



# Custom Dataset for loading images and labels
class SegmentationDataset(Dataset):
    def __init__(self, image_files, label_files, transform=None):
        self.image_files = image_files
        self.label_files = label_files
        self.transform = transform

    def __len__(self):
        return len(self.image_files)


    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        label_path = self.image_files[idx].replace('.jpg', '.png').replace('color', 'label')
        image = Image.open(img_path).convert('RGB')
        label = Image.open(label_path).convert('L')  # Load label as grayscale

        if self.transform:
            image = self.transform(image)
            
        label = label.resize((256, 256), Image.NEAREST)

        # Convert label to tensor
        label = torch.tensor(np.array(label), dtype=torch.long)  # Shape: (H, W)

        # # Map pixel values to class indices
        # label = torch.where(label == 38, 1, label)
        # label = torch.where(label == 75, 2, label)
        # label = torch.where(label == 255, 0, label)
        # label = torch.where(label == 0, 0, label)  # Ensure 0 stays as class 0

        mapping = {38: 1, 75: 2, 255: 0, 0: 0}
        label = torch.tensor(np.vectorize(mapping.get)(np.array(label)), dtype=torch.long)

        

        return image, label  # No one-hot encoding




# Transformations
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
])


# Load segmentation dataset with first 100 images
segmentation_dataset = SegmentationDataset(
    image_files=train_images,
    label_files=train_masks,
    transform=transform
)
segmentation_dataloader = DataLoader(segmentation_dataset, batch_size=batchSize, shuffle=True)

# Load the pre-trained autoencoder
autoencoder = Autoencoder()
autoencoder.load_state_dict(torch.load('Data/Output/Models/updated_working_autoencoder_epoch_30.pth'))

# Initialize the segmentation model
# segmentation_model = SegmentationDecoder(autoencoder.encoder)

# Define loss and optimizer for segmentation
segmentation_model = SegmentationDecoder(autoencoder.encoder)
#class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0])  # Adjust these values as needed
segmentation_criterion = nn.CrossEntropyLoss()#(weight=class_weights)
# Example of class weights (higher weight for less frequent classes)
segmentation_optimizer = optim.Adam(segmentation_model.decoder.parameters(), lr=1e-3, weight_decay=1e-4)

# Training the Segmentation Model

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
segmentation_model.to(device)
# scheduler = torch.optim.lr_scheduler.StepLR(segmentation_optimizer, step_size=3, gamma=0.5)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(segmentation_optimizer, T_max=10, eta_min=1e-5)

scaler = torch.cuda.amp.GradScaler()

# num_epochs = 100
# for epoch in range(num_epochs):
#     print(f"Epoch [{epoch+1}/{num_epochs}] Running:")
#     for images, labels in segmentation_dataloader:
#         images, labels = images.to(device), labels.to(device)

#         segmentation_optimizer.zero_grad()

#         # Use autocast for mixed precision training
#         with autocast():  
#             outputs = segmentation_model(images)  # Forward pass
#             loss = segmentation_criterion(outputs, labels)  # Compute loss

#         # Scale the loss and backpropagate
#         scaler.scale(loss).backward()

#         # Step the optimizer with scaled gradients
#         scaler.step(segmentation_optimizer)
        
#         # Update the scaler for next iteration
#         scaler.update()
        
#     scheduler.step()


      
#     print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")
#     torch.save(segmentation_model.state_dict(), f'Data/Output/Models/improved_segmentation_model_epoch_{epoch+1}.pth')

import matplotlib.pyplot as plt
import torch
from torchvision import transforms
from PIL import Image
import numpy as np


# Load the trained segmentation model
segmentation_model = SegmentationDecoder(autoencoder.encoder)

segmentation_model.eval()  # Set model to evaluation mode

# Define the transformation (matching training)
transform = transforms.Compose([
    transforms.Resize((500, 500)),
    transforms.ToTensor()
])

# Function to make predictions and visualize results
def visualize_segmentation(image, label):
    input_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension & move to device

    # Forward pass through the model
    with torch.no_grad():
        output = segmentation_model(input_tensor)  # Shape: (1, num_classes, H, W)
        output = torch.softmax(output, dim=1)  # Apply softmax over class dimension

        outputs = segmentation_model(images)  # Shape: (Batch, 4, H, W)
        print("Output Tensor Stats:")
        print("Min:", outputs.min().item(), "Max:", outputs.max().item(), "Mean:", outputs.mean().item())

        predicted_mask = torch.argmax(outputs, dim=1).cpu().numpy()
        print("Unique values in predicted mask:", np.unique(predicted_mask))

        print("Output Shape:", outputs.shape)  # Should be (batch, num_classes, H, W)
        print("Labels Shape:", labels.shape)  # Should be (batch, H, W)



        # Convert output to class labels
        predicted_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # Shape: (H, W)

    # Convert label to numpy array
    label_np = np.array(label.resize((500, 500)))  # Resize label to match output size

    # Plot results
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    axes[0].imshow(image)
    axes[0].set_title("Input Image")
    axes[0].axis("off")

    axes[1].imshow(label_np, cmap='viridis', vmin=0, vmax=np.max(label_np))
    axes[1].set_title("Ground Truth Label")
    axes[1].axis("off")

    axes[2].imshow(predicted_mask, cmap='viridis', vmin=0, vmax=np.max(predicted_mask))
    axes[2].set_title("Model Prediction")
    axes[2].axis("off")

    # Print unique values in predicted mask
    unique_values = np.unique(predicted_mask)
    print("Unique predicted values:", unique_values)

    plt.show()

# Load sample image and label
sampleImg = Image.open("Data/Output/Augmented/color/aug_Birman_159.jpg").convert('RGB')
sampleLabel = Image.open("Data/Output/Augmented/label/aug_Birman_159.png").convert('L')  # (grayscale)

# Run visualization
visualize_segmentation(sampleImg, sampleLabel)

sampleImg = Image.open("Data/Output/Augmented/color/aug_Birman_159.jpg").convert('RGB')
sampleLabel = Image.open("Data/Output/Augmented/label/aug_Birman_159.png").convert('L') # (grayscale)
visualize_segmentation(sampleImg, sampleLabel)

"""#### c) CLIP (incomplete)"""

# from transformers import CLIPProcessor, CLIPModel
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import numpy as np
# from torch.utils.data import Dataset, DataLoader
# from torchvision import transforms
# from PIL import Image
# import os

# # Load CLIP Model
# device = "cuda" if torch.cuda.is_available() else "cpu"
# model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
# processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# # Directory to save CLIP features
# clip_features_dir = "clipTrainFeatures"
# os.makedirs(clip_features_dir, exist_ok=True)

# # Extract and save CLIP features
# def extract_clip_features(image_path):
#     image = Image.open(image_path).convert("RGB")
#     inputs = processor(images=image, return_tensors="pt").to(device)
#     with torch.no_grad():
#         features = model.get_image_features(**inputs)
#     features = features / features.norm(p=2, dim=-1, keepdim=True)  # Normalize
#     return features.cpu()

# clip_features_trainval = {}

# for image_name in trainval_images:
#     image_path = os.path.join("Data/Input/TrainVal/color", image_name)
#     clip_features = extract_clip_features(image_path)

#     # Save each feature tensor separately
#     feature_path = os.path.join(clip_features_dir, f"{image_name}.pt")
#     torch.save(clip_features, feature_path)

#     clip_features_trainval[image_name] = feature_path  # Store path reference

# print(f"Saved {len(clip_features_trainval)} CLIP feature tensors.")


# # Custom Dataset for Segmentation
# class SegmentationDataset(Dataset):
#     def __init__(self, image_files, clip_feature_dir, transform=None):
#         self.image_files = image_files
#         self.clip_feature_dir = clip_feature_dir
#         self.transform = transform

#     def __len__(self):
#         return len(self.image_files)

#     def __getitem__(self, idx):
#         image_name = self.image_files[idx]

#         # Load CLIP features
#         feature_path = os.path.join(self.clip_feature_dir, f"{image_name}.pt")
#         clip_features = torch.load(feature_path)

#         # Load label (Segmentation mask)
#         label_path = os.path.join("Data/Input/TrainVal/label", image_name.replace('.jpg', '.png'))
#         label = Image.open(label_path).convert('L')  # Grayscale

#         if self.transform:
#             label = self.transform(label)

#         label = torch.tensor(np.array(label), dtype=torch.long)  # Convert to tensor

#         # Map pixel values to class indices
#         label = torch.where(label == 38, 1, label)
#         label = torch.where(label == 75, 2, label)
#         label = torch.where(label == 255, 3, label)
#         label = torch.where(label == 0, 0, label)  # Background class

#         return clip_features.squeeze(0), label


# # Define Transformations
# transform = transforms.Compose([
#     transforms.Resize((256, 256), interpolation=Image.NEAREST),
#     transforms.ToTensor()
# ])

# # Load Dataset & DataLoader
# batch_size = 16
# segmentation_dataset = SegmentationDataset(
#     image_files=trainval_images, 
#     clip_feature_dir=clip_features_dir, 
#     transform=transform
# )
# segmentation_dataloader = DataLoader(segmentation_dataset, batch_size=batch_size, shuffle=True)


# # Segmentation Model
# class SegmentationDecoder(nn.Module):
#     def __init__(self, clip_feature_dim=512, num_classes=4):
#         super(SegmentationDecoder, self).__init__()

#         # CLIP feature integration layer
#         self.clip_fc = nn.Linear(clip_feature_dim, 256)  # Reduce CLIP feature size

#         # Convolutional layers for segmentation
#         self.decoder = nn.Sequential(
#             nn.ConvTranspose2d(256, 128, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
#             nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),
#             nn.ReLU(),
#             nn.ConvTranspose2d(64, num_classes, kernel_size=3, stride=1, padding=1)  # Output mask
#         )

#     def forward(self, clip_features):
#         # Process CLIP features
#         clip_features = self.clip_fc(clip_features)  # Reduce dimension
#         clip_features = clip_features.unsqueeze(-1).unsqueeze(-1)  # Reshape for CNN input
#         clip_features = clip_features.expand(-1, -1, 32, 32)  # Expand to spatial feature map

#         # Decode into segmentation mask
#         segmentation_output = self.decoder(clip_features)
#         return segmentation_output


# # Initialize Model, Loss, Optimizer
# segmentation_model = SegmentationDecoder().to(device)
# segmentation_criterion = nn.CrossEntropyLoss()
# segmentation_optimizer = optim.Adam(segmentation_model.parameters(), lr=1e-3)
# scheduler = torch.optim.lr_scheduler.StepLR(segmentation_optimizer, step_size=5, gamma=0.5)

# # Mixed Precision Training
# scaler = torch.cuda.amp.GradScaler()


# # Training Loop
# num_epochs = 30
# for epoch in range(num_epochs):
#     segmentation_model.train()
#     running_loss = 0.0

#     print(f"Epoch [{epoch+1}/{num_epochs}] Running:")
    
#     for clip_features, labels in segmentation_dataloader:
#         clip_features, labels = clip_features.to(device), labels.to(device)

#         segmentation_optimizer.zero_grad()

#         # Mixed precision training
#         with torch.cuda.amp.autocast():
#             outputs = segmentation_model(clip_features)  # Forward pass
#             loss = segmentation_criterion(outputs, labels)  # Compute loss

#         # Scale the loss and backpropagate
#         scaler.scale(loss).backward()

#         # Step the optimizer with scaled gradients
#         scaler.step(segmentation_optimizer)
#         scaler.update()

#         running_loss += loss.item()

#     scheduler.step()
    
#     avg_loss = running_loss / len(segmentation_dataloader)
#     print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")

#     # Save model checkpoint
#     torch.save(segmentation_model.state_dict(), f'Data/Output/Models/clip_segmentation_model_epoch_{epoch+1}.pth')

# print("Training complete!")


'''
# WORKING CLIP

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os
import open_clip  # Alternative to Hugging Face's CLIP

# Load CLIP Model from open_clip
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms("ViT-B-32", pretrained="openai")
model = model.to(device)
tokenizer = open_clip.get_tokenizer("ViT-B-32")

# Directory to save CLIP features
clip_features_dir = "Data/Output/Models/clipTrainFeatures"
# os.makedirs(clip_features_dir, exist_ok=True)

# # Extract and save CLIP features
# def extract_clip_features(image_path):
#     image = Image.open(image_path).convert("RGB")
#     image_tensor = preprocess_val(image).unsqueeze(0).to(device)  # Use open_clip's preprocessing
#     with torch.no_grad():
#         features = model.encode_image(image_tensor)  # Extract image features
#     features = features / features.norm(p=2, dim=-1, keepdim=True)  # Normalize
#     return features.cpu()

# clip_features_trainval = {}

# for image_name in trainval_images:
#     image_path = image_name
#     clip_features = extract_clip_features(image_path)

#     # Save each feature tensor separately
#     feature_path = os.path.join(clip_features_dir, f"{image_name.split('/')[-1].split('.')[0]}.pt")
#     torch.save(clip_features, feature_path)

#     clip_features_trainval[image_name] = feature_path  # Store path reference

clip_features_trainval = {
    image_name: torch.load(os.path.join(clip_features_dir, f"{image_name.split('/')[-1].split('.')[0]}.pt"))
    for image_name in trainval_images
}

print("*****************************************************")
print(f"Loaded {len(clip_features_trainval)} CLIP feature tensors.")
# print(f"Loaded {clip_features_trainval} CLIP feature tensors.")
print("*****************************************************")


# Custom Dataset for Segmentation
class SegmentationDataset(Dataset):
    def __init__(self, image_files, label_files, clip_features_dict, transform=None):
        self.image_files = image_files
        self.label_files = label_files
        self.clip_features_dict = clip_features_dict  # Store CLIP features
        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        label_path = img_path.replace('.jpg', '.png').replace('color', 'label')

        # Load image
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)  # Apply transformations
        else:
            image = transforms.ToTensor()(image)  # Convert manually if transform is None

        # Load CLIP feature tensor
        image_name = img_path
        clip_feature = self.clip_features_dict[image_name]  # Shape: (512,)

        # Load and process label
        label = Image.open(label_path).convert('L')
        label = label.resize((256, 256), Image.NEAREST)
        label = torch.tensor(np.array(label), dtype=torch.long)

        # Convert label classes to match expected format
        label = torch.where(label == 38, 1, label)
        label = torch.where(label == 75, 2, label)
        label = torch.where(label == 255, 3, label)
        label = torch.where(label == 0, 0, label)

        return image, clip_feature, label  # Return both image & CLIP feature




# Define Transformations
transform = transforms.Compose([
    transforms.Resize((256, 256), interpolation=Image.NEAREST),
    transforms.ToTensor()
])

# Load Dataset & DataLoader
batch_size = 16


segmentation_dataset = SegmentationDataset(
    image_files=train_images,
    label_files=train_masks,
    clip_features_dict=clip_features_trainval,  # Use CLIP features
    transform=None  # No need for image transformations
)


segmentation_dataloader = DataLoader(segmentation_dataset, batch_size=batch_size, shuffle=True)



# Segmentation Model
class SegmentationDecoder(nn.Module):
    def __init__(self, clip_feature_dim=512, num_classes=4):
        super(SegmentationDecoder, self).__init__()

        # Project CLIP features
        self.clip_fc = nn.Linear(clip_feature_dim, 128)  

        # CNN Feature Extractor
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128 + 128, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True),  # Ensure fixed output size
            nn.ConvTranspose2d(64, num_classes, kernel_size=3, stride=1, padding=1),
        )


    def forward(self, image, clip_features):
        cnn_features = self.encoder(image)
        clip_features = self.clip_fc(clip_features).view(clip_features.shape[0], 128, 1, 1)
        clip_features = clip_features.expand(-1, -1, cnn_features.shape[2], cnn_features.shape[3])

        fusion = torch.cat([cnn_features, clip_features], dim=1)
        segmentation_output = self.decoder(fusion)
        return segmentation_output



# Initialize Model, Loss, Optimizer
segmentation_model = SegmentationDecoder().to(device)
segmentation_criterion = nn.CrossEntropyLoss()
segmentation_optimizer = optim.Adam(segmentation_model.parameters(), lr=1e-3)
scheduler = torch.optim.lr_scheduler.StepLR(segmentation_optimizer, step_size=5, gamma=0.5)

# Mixed Precision Training
scaler = torch.cuda.amp.GradScaler()


# Training Loop
num_epochs = 30
for epoch in range(num_epochs):
    segmentation_model.train()
    running_loss = 0.0

    print(f"Epoch [{epoch+1}/{num_epochs}] Running:")
    
    for images, clip_features, labels in segmentation_dataloader:  # Unpack correctly
        images, clip_features, labels = images.to(device), clip_features.to(device), labels.to(device)

        segmentation_optimizer.zero_grad()

        # Mixed precision training
        with torch.cuda.amp.autocast(): 
            outputs = segmentation_model(images, clip_features)  # Now correctly passing 2 args
            loss = segmentation_criterion(outputs, labels)  # Compute loss

        # Scale the loss and backpropagate
        scaler.scale(loss).backward()
        scaler.step(segmentation_optimizer)
        scaler.update()

        running_loss += loss.item()


    scheduler.step()
    
    avg_loss = running_loss / len(segmentation_dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")

    torch.save(segmentation_model.state_dict(), f'Data/Output/Models/clip_segmentation_model_epoch_{epoch+1}.pth')



print("Training complete!")

'''


   
